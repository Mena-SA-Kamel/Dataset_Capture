2020-07-20 17:55:41.894455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.895009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 17:55:41.895082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 17:55:41.895107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 17:55:41.895128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 17:55:41.895157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 17:55:41.895176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 17:55:41.895190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 17:55:41.895204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 17:55:41.895274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.895855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.896379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 17:55:41.897003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.897505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 17:55:41.897552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 17:55:41.897569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 17:55:41.897579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 17:55:41.897592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 17:55:41.897605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 17:55:41.897617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 17:55:41.897630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 17:55:41.897677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.898197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.898718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 17:55:41.898779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-20 17:55:41.898796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-20 17:55:41.898822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-20 17:55:41.898912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.899449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 17:55:41.899944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-20 17:55:42.093329: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-20 17:55:42.093484: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200720T1755/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-20 17:56:43.710185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 17:56:49.729011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
  2/309 [..............................] - ETA: 1:58:47 - loss: 0.2282 - rpn_loss: 0.2282/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.460493). Check your callbacks.
  % (hook_name, delta_t_median), RuntimeWarning)
309/309 [==============================] - 147s 474ms/step - loss: 0.1020 - rpn_loss: 0.1020 - val_loss: 0.0871 - val_rpn_loss: 0.0948
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0959 - rpn_loss: 0.0959 - val_loss: 0.1026 - val_rpn_loss: 0.0936
Epoch 3/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0947 - rpn_loss: 0.0947 - val_loss: 0.0943 - val_rpn_loss: 0.0922
Epoch 4/500
309/309 [==============================] - 135s 435ms/step - loss: 0.0951 - rpn_loss: 0.0951 - val_loss: 0.0925 - val_rpn_loss: 0.0912
Epoch 5/500
309/309 [==============================] - 91s 295ms/step - loss: 0.0935 - rpn_loss: 0.0935 - val_loss: 0.0864 - val_rpn_loss: 0.0900
Epoch 6/500
309/309 [==============================] - 126s 407ms/step - loss: 0.0924 - rpn_loss: 0.0924 - val_loss: 0.0819 - val_rpn_loss: 0.0898
Epoch 7/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0920 - rpn_loss: 0.0920 - val_loss: 0.1015 - val_rpn_loss: 0.0909
Epoch 8/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0914 - rpn_loss: 0.0913 - val_loss: 0.0885 - val_rpn_loss: 0.0886
Epoch 9/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0915 - rpn_loss: 0.0915 - val_loss: 0.0940 - val_rpn_loss: 0.0891
Epoch 10/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0916 - rpn_loss: 0.0916 - val_loss: 0.0900 - val_rpn_loss: 0.0904
Epoch 11/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0916 - rpn_loss: 0.0916 - val_loss: 0.0910 - val_rpn_loss: 0.0891
Epoch 12/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0909 - rpn_loss: 0.0909 - val_loss: 0.0873 - val_rpn_loss: 0.0890
Epoch 13/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0918 - rpn_loss: 0.0918 - val_loss: 0.0869 - val_rpn_loss: 0.0903
Epoch 14/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0903 - rpn_loss: 0.0903 - val_loss: 0.0887 - val_rpn_loss: 0.0879
Epoch 15/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0897 - rpn_loss: 0.0897 - val_loss: 0.0931 - val_rpn_loss: 0.0878
Epoch 16/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0888 - rpn_loss: 0.0888 - val_loss: 0.0872 - val_rpn_loss: 0.0860
Epoch 17/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0889 - rpn_loss: 0.0889 - val_loss: 0.0880 - val_rpn_loss: 0.0880
Epoch 18/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0890 - rpn_loss: 0.0890 - val_loss: 0.0927 - val_rpn_loss: 0.0859
Epoch 19/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0883 - rpn_loss: 0.0883 - val_loss: 0.0879 - val_rpn_loss: 0.0858
Epoch 20/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0870 - rpn_loss: 0.0870 - val_loss: 0.0860 - val_rpn_loss: 0.0866
Epoch 21/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0864 - rpn_loss: 0.0864 - val_loss: 0.0866 - val_rpn_loss: 0.0848
Epoch 22/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0858 - rpn_loss: 0.0858 - val_loss: 0.0857 - val_rpn_loss: 0.0840
Epoch 23/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0850 - rpn_loss: 0.0850 - val_loss: 0.0827 - val_rpn_loss: 0.0832
Epoch 24/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0848 - rpn_loss: 0.0848 - val_loss: 0.0819 - val_rpn_loss: 0.0827
Epoch 25/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0836 - rpn_loss: 0.0836 - val_loss: 0.0850 - val_rpn_loss: 0.0839
Epoch 26/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0836 - rpn_loss: 0.0835 - val_loss: 0.0806 - val_rpn_loss: 0.0821
Epoch 27/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0841 - rpn_loss: 0.0841 - val_loss: 0.0747 - val_rpn_loss: 0.0799
Epoch 28/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0831 - rpn_loss: 0.0831 - val_loss: 0.0944 - val_rpn_loss: 0.0820
Epoch 29/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0826 - rpn_loss: 0.0826 - val_loss: 0.0782 - val_rpn_loss: 0.0819
Epoch 30/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0819 - rpn_loss: 0.0819 - val_loss: 0.0747 - val_rpn_loss: 0.0795
Epoch 31/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0806 - rpn_loss: 0.0806 - val_loss: 0.0774 - val_rpn_loss: 0.0798
Epoch 32/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0809 - rpn_loss: 0.0808 - val_loss: 0.0806 - val_rpn_loss: 0.0788
Epoch 33/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0801 - rpn_loss: 0.0801 - val_loss: 0.0754 - val_rpn_loss: 0.0773
Epoch 34/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0803 - rpn_loss: 0.0803 - val_loss: 0.0812 - val_rpn_loss: 0.0796
Epoch 35/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0796 - rpn_loss: 0.0796 - val_loss: 0.0674 - val_rpn_loss: 0.0791
Epoch 36/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0790 - rpn_loss: 0.0790 - val_loss: 0.0862 - val_rpn_loss: 0.0781
Epoch 37/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0787 - rpn_loss: 0.0787 - val_loss: 0.0825 - val_rpn_loss: 0.0787
Epoch 38/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0793 - rpn_loss: 0.0793 - val_loss: 0.0688 - val_rpn_loss: 0.0763
Epoch 39/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0779 - rpn_loss: 0.0779 - val_loss: 0.0724 - val_rpn_loss: 0.0757
Epoch 40/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0789 - rpn_loss: 0.0788 - val_loss: 0.0840 - val_rpn_loss: 0.0779
Epoch 41/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0780 - rpn_loss: 0.0780 - val_loss: 0.0702 - val_rpn_loss: 0.0767
Epoch 42/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0780 - rpn_loss: 0.0780 - val_loss: 0.0788 - val_rpn_loss: 0.0763
Epoch 43/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0779 - rpn_loss: 0.0779 - val_loss: 0.0885 - val_rpn_loss: 0.0769
Epoch 44/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0775 - rpn_loss: 0.0775 - val_loss: 0.0751 - val_rpn_loss: 0.0762
Epoch 45/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0769 - rpn_loss: 0.0768 - val_loss: 0.0833 - val_rpn_loss: 0.0783
Epoch 46/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0767 - rpn_loss: 0.0767 - val_loss: 0.0859 - val_rpn_loss: 0.0759
Epoch 47/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0763 - rpn_loss: 0.0762 - val_loss: 0.0786 - val_rpn_loss: 0.0740
Epoch 48/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0770 - rpn_loss: 0.0770 - val_loss: 0.0706 - val_rpn_loss: 0.0760
Epoch 49/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0763 - rpn_loss: 0.0763 - val_loss: 0.0758 - val_rpn_loss: 0.0750
Epoch 50/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0760 - rpn_loss: 0.0759 - val_loss: 0.0753 - val_rpn_loss: 0.0753
Epoch 51/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0762 - rpn_loss: 0.0762 - val_loss: 0.0837 - val_rpn_loss: 0.0752
Epoch 52/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0768 - rpn_loss: 0.0768 - val_loss: 0.0734 - val_rpn_loss: 0.0744
Epoch 53/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0754 - rpn_loss: 0.0754 - val_loss: 0.0735 - val_rpn_loss: 0.0759
Epoch 54/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0761 - rpn_loss: 0.0761 - val_loss: 0.0794 - val_rpn_loss: 0.0758
Epoch 55/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0767 - rpn_loss: 0.0767 - val_loss: 0.0755 - val_rpn_loss: 0.0742
Epoch 56/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0747 - rpn_loss: 0.0747 - val_loss: 0.0734 - val_rpn_loss: 0.0738
Epoch 57/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0758 - rpn_loss: 0.0758 - val_loss: 0.0691 - val_rpn_loss: 0.0747
Epoch 58/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0756 - rpn_loss: 0.0756 - val_loss: 0.0854 - val_rpn_loss: 0.0754
Epoch 59/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0753 - rpn_loss: 0.0753 - val_loss: 0.0767 - val_rpn_loss: 0.0745
Epoch 60/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0752 - rpn_loss: 0.0752 - val_loss: 0.0721 - val_rpn_loss: 0.0755
Epoch 61/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0748 - rpn_loss: 0.0748 - val_loss: 0.0824 - val_rpn_loss: 0.0737
Epoch 62/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0754 - rpn_loss: 0.0754 - val_loss: 0.0751 - val_rpn_loss: 0.0740
Epoch 63/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0750 - rpn_loss: 0.0750 - val_loss: 0.0740 - val_rpn_loss: 0.0741
Epoch 64/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0753 - rpn_loss: 0.0753 - val_loss: 0.0705 - val_rpn_loss: 0.0754
Epoch 65/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0750 - rpn_loss: 0.0750 - val_loss: 0.0748 - val_rpn_loss: 0.0734
Epoch 66/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0750 - rpn_loss: 0.0750 - val_loss: 0.0730 - val_rpn_loss: 0.0739
Epoch 67/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0745 - rpn_loss: 0.0745 - val_loss: 0.0690 - val_rpn_loss: 0.0740
Epoch 68/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0743 - rpn_loss: 0.0743 - val_loss: 0.0724 - val_rpn_loss: 0.0737
Epoch 69/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0740 - rpn_loss: 0.0740 - val_loss: 0.0734 - val_rpn_loss: 0.0752
Epoch 70/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0747 - rpn_loss: 0.0747 - val_loss: 0.0775 - val_rpn_loss: 0.0737
Epoch 71/500
  4/309 [..............................] - ETA: 1:23 - loss: 0.0722 - rpn_loss: 0.0722