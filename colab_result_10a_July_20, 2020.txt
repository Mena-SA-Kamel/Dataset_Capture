WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:624: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:626: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-20 20:38:00.049999: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-07-20 20:38:00.050258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3269480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-20 20:38:00.050289: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-20 20:38:00.052019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-20 20:38:00.159787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.160555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32699c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-20 20:38:00.160619: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-20 20:38:00.160873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.161395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 20:38:00.161694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:00.163189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 20:38:00.164646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 20:38:00.164975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 20:38:00.166397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 20:38:00.167102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 20:38:00.170078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:38:00.170198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.171007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.171531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 20:38:00.171593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:00.172925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-20 20:38:00.172957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-20 20:38:00.172965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-20 20:38:00.173081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.173598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.174149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1423: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-20 20:38:05.497611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.498240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 20:38:05.498327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:05.498347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 20:38:05.498360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 20:38:05.498375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 20:38:05.498389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 20:38:05.498404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 20:38:05.498418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:38:05.498487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.499045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.499537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 20:38:05.500229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.500775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 20:38:05.500836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:05.500892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 20:38:05.500902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 20:38:05.500915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 20:38:05.500928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 20:38:05.500940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 20:38:05.500953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:38:05.501023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.501552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.502070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 20:38:05.502125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-20 20:38:05.502137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-20 20:38:05.502144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-20 20:38:05.502235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.502785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.503266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-20 20:38:05.696126: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-20 20:38:05.696306: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200720T2038/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-20 20:39:11.907759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:39:16.569070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
  2/309 [..............................] - ETA: 2:00:57 - loss: 0.2374 - rpn_loss: 0.2374/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.496090). Check your callbacks.
  % (hook_name, delta_t_median), RuntimeWarning)
309/309 [==============================] - 151s 489ms/step - loss: 0.1032 - rpn_loss: 0.1032 - val_loss: 0.0920 - val_rpn_loss: 0.0944
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0952 - rpn_loss: 0.0952 - val_loss: 0.0906 - val_rpn_loss: 0.0923
Epoch 3/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0954 - rpn_loss: 0.0954 - val_loss: 0.0877 - val_rpn_loss: 0.0921
Epoch 4/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0934 - rpn_loss: 0.0933 - val_loss: 0.0938 - val_rpn_loss: 0.0908
Epoch 5/500
309/309 [==============================] - 92s 297ms/step - loss: 0.0913 - rpn_loss: 0.0913 - val_loss: 0.0906 - val_rpn_loss: 0.0909
Epoch 6/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0944 - val_rpn_loss: 0.0901
Epoch 7/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0922 - rpn_loss: 0.0922 - val_loss: 0.0900 - val_rpn_loss: 0.0901
Epoch 8/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0873 - val_rpn_loss: 0.0891
Epoch 9/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0915 - rpn_loss: 0.0915 - val_loss: 0.0916 - val_rpn_loss: 0.0887
Epoch 10/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0913 - rpn_loss: 0.0913 - val_loss: 0.0900 - val_rpn_loss: 0.0912
Epoch 11/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0909 - rpn_loss: 0.0909 - val_loss: 0.0895 - val_rpn_loss: 0.0891
Epoch 12/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0912 - rpn_loss: 0.0911 - val_loss: 0.0851 - val_rpn_loss: 0.0885
Epoch 13/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0891 - rpn_loss: 0.0890 - val_loss: 0.0843 - val_rpn_loss: 0.0866
Epoch 14/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0875 - rpn_loss: 0.0875 - val_loss: 0.0795 - val_rpn_loss: 0.0840
Epoch 15/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0839 - rpn_loss: 0.0839 - val_loss: 0.0843 - val_rpn_loss: 0.0837
Epoch 16/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0831 - rpn_loss: 0.0831 - val_loss: 0.0863 - val_rpn_loss: 0.0817
Epoch 17/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0837 - rpn_loss: 0.0837 - val_loss: 0.0804 - val_rpn_loss: 0.0837
Epoch 18/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0822 - rpn_loss: 0.0822 - val_loss: 0.0778 - val_rpn_loss: 0.0814
Epoch 19/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0803 - rpn_loss: 0.0803 - val_loss: 0.0781 - val_rpn_loss: 0.0787
Epoch 20/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0794 - rpn_loss: 0.0794 - val_loss: 0.0736 - val_rpn_loss: 0.0771
Epoch 21/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0776 - rpn_loss: 0.0776 - val_loss: 0.0785 - val_rpn_loss: 0.0763
Epoch 22/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0735 - rpn_loss: 0.0735 - val_loss: 0.0751 - val_rpn_loss: 0.0749
Epoch 23/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0701 - rpn_loss: 0.0701 - val_loss: 0.0774 - val_rpn_loss: 0.0725
Epoch 24/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0696 - rpn_loss: 0.0696 - val_loss: 0.0722 - val_rpn_loss: 0.0680
Epoch 25/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0693 - rpn_loss: 0.0693 - val_loss: 0.0623 - val_rpn_loss: 0.0690
Epoch 26/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0688 - rpn_loss: 0.0688 - val_loss: 0.0762 - val_rpn_loss: 0.0710
Epoch 27/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0702 - rpn_loss: 0.0702 - val_loss: 0.0719 - val_rpn_loss: 0.0737
Epoch 28/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0710 - rpn_loss: 0.0709 - val_loss: 0.0742 - val_rpn_loss: 0.0733
Epoch 29/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0701 - rpn_loss: 0.0700 - val_loss: 0.0594 - val_rpn_loss: 0.0687
Epoch 30/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0676 - rpn_loss: 0.0676 - val_loss: 0.0708 - val_rpn_loss: 0.0684
Epoch 31/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0645 - rpn_loss: 0.0645 - val_loss: 0.0656 - val_rpn_loss: 0.0660
Epoch 32/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0623 - rpn_loss: 0.0623 - val_loss: 0.0607 - val_rpn_loss: 0.0641
Epoch 33/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0630 - rpn_loss: 0.0629 - val_loss: 0.0622 - val_rpn_loss: 0.0645
Epoch 34/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0632 - rpn_loss: 0.0632 - val_loss: 0.0547 - val_rpn_loss: 0.0657
Epoch 35/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0625 - rpn_loss: 0.0625 - val_loss: 0.0778 - val_rpn_loss: 0.0673
Epoch 36/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0641 - rpn_loss: 0.0641 - val_loss: 0.0703 - val_rpn_loss: 0.0683
Epoch 37/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0633 - rpn_loss: 0.0633 - val_loss: 0.0621 - val_rpn_loss: 0.0659
Epoch 38/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0611 - rpn_loss: 0.0611 - val_loss: 0.0620 - val_rpn_loss: 0.0641
Epoch 39/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0589 - rpn_loss: 0.0589 - val_loss: 0.0578 - val_rpn_loss: 0.0626
Epoch 40/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0586 - rpn_loss: 0.0586 - val_loss: 0.0688 - val_rpn_loss: 0.0602
Epoch 41/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0585 - rpn_loss: 0.0585 - val_loss: 0.0710 - val_rpn_loss: 0.0631
Epoch 42/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0570 - rpn_loss: 0.0570 - val_loss: 0.0607 - val_rpn_loss: 0.0631
Epoch 43/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0588 - rpn_loss: 0.0587 - val_loss: 0.0662 - val_rpn_loss: 0.0657
Epoch 44/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0604 - rpn_loss: 0.0603 - val_loss: 0.0643 - val_rpn_loss: 0.0630
Epoch 45/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0594 - rpn_loss: 0.0594 - val_loss: 0.0632 - val_rpn_loss: 0.0627
Epoch 46/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0564 - rpn_loss: 0.0564 - val_loss: 0.0592 - val_rpn_loss: 0.0606
Epoch 47/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0542 - rpn_loss: 0.0542 - val_loss: 0.0640 - val_rpn_loss: 0.0624
Epoch 48/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0545 - rpn_loss: 0.0545 - val_loss: 0.0583 - val_rpn_loss: 0.0585
Epoch 49/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0547 - rpn_loss: 0.0547 - val_loss: 0.0644 - val_rpn_loss: 0.0590
Epoch 50/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0531 - rpn_loss: 0.0531 - val_loss: 0.0644 - val_rpn_loss: 0.0618
Epoch 51/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0550 - rpn_loss: 0.0550 - val_loss: 0.0582 - val_rpn_loss: 0.0632
Epoch 52/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0552 - rpn_loss: 0.0552 - val_loss: 0.0539 - val_rpn_loss: 0.0635
Epoch 53/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0565 - rpn_loss: 0.0565 - val_loss: 0.0598 - val_rpn_loss: 0.0605
Epoch 54/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0536 - rpn_loss: 0.0536 - val_loss: 0.0528 - val_rpn_loss: 0.0607
Epoch 55/500
309/309 [==============================] - 141s 455ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0513 - val_rpn_loss: 0.0588
Epoch 56/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0508 - rpn_loss: 0.0507 - val_loss: 0.0599 - val_rpn_loss: 0.0595
Epoch 57/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0514 - rpn_loss: 0.0514 - val_loss: 0.0596 - val_rpn_loss: 0.0585
Epoch 58/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0614 - val_rpn_loss: 0.0607
Epoch 59/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0536 - val_rpn_loss: 0.0613
Epoch 60/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0540 - rpn_loss: 0.0540 - val_loss: 0.0622 - val_rpn_loss: 0.0614
Epoch 61/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0530 - rpn_loss: 0.0530 - val_loss: 0.0649 - val_rpn_loss: 0.0621
Epoch 62/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0614 - val_rpn_loss: 0.0612
Epoch 63/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0584 - val_rpn_loss: 0.0608
Epoch 64/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0472 - rpn_loss: 0.0472 - val_loss: 0.0550 - val_rpn_loss: 0.0560
Epoch 65/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0468 - rpn_loss: 0.0468 - val_loss: 0.0660 - val_rpn_loss: 0.0586
Epoch 66/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0482 - rpn_loss: 0.0482 - val_loss: 0.0556 - val_rpn_loss: 0.0586
Epoch 67/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0679 - val_rpn_loss: 0.0633
Epoch 68/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0667 - val_rpn_loss: 0.0652
Epoch 69/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0509 - rpn_loss: 0.0509 - val_loss: 0.0706 - val_rpn_loss: 0.0615
Epoch 70/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0483 - rpn_loss: 0.0483 - val_loss: 0.0585 - val_rpn_loss: 0.0609
Epoch 71/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0466 - rpn_loss: 0.0466 - val_loss: 0.0497 - val_rpn_loss: 0.0594
Epoch 72/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0463 - rpn_loss: 0.0462 - val_loss: 0.0635 - val_rpn_loss: 0.0560
Epoch 73/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0453 - rpn_loss: 0.0453 - val_loss: 0.0612 - val_rpn_loss: 0.0623
Epoch 74/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0450 - rpn_loss: 0.0449 - val_loss: 0.0504 - val_rpn_loss: 0.0617
Epoch 75/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0476 - rpn_loss: 0.0476 - val_loss: 0.0643 - val_rpn_loss: 0.0615
Epoch 76/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0579 - val_rpn_loss: 0.0646
Epoch 77/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0503 - rpn_loss: 0.0503 - val_loss: 0.0699 - val_rpn_loss: 0.0620
Epoch 78/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0474 - rpn_loss: 0.0474 - val_loss: 0.0564 - val_rpn_loss: 0.0576
Epoch 79/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0459 - rpn_loss: 0.0459 - val_loss: 0.0596 - val_rpn_loss: 0.0592
Epoch 80/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0443 - rpn_loss: 0.0443 - val_loss: 0.0672 - val_rpn_loss: 0.0579
Epoch 81/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0449 - rpn_loss: 0.0449 - val_loss: 0.0695 - val_rpn_loss: 0.0625
Epoch 82/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0430 - rpn_loss: 0.0430 - val_loss: 0.0595 - val_rpn_loss: 0.0632
Epoch 83/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0450 - rpn_loss: 0.0450 - val_loss: 0.0771 - val_rpn_loss: 0.0641
Epoch 84/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0479 - rpn_loss: 0.0478 - val_loss: 0.0706 - val_rpn_loss: 0.0656
Epoch 85/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0483 - rpn_loss: 0.0483 - val_loss: 0.0485 - val_rpn_loss: 0.0614
Epoch 86/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0457 - rpn_loss: 0.0457 - val_loss: 0.0723 - val_rpn_loss: 0.0609
Epoch 87/500
309/309 [==============================] - 141s 455ms/step - loss: 0.0427 - rpn_loss: 0.0426 - val_loss: 0.0515 - val_rpn_loss: 0.0610
Epoch 88/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0431 - rpn_loss: 0.0431 - val_loss: 0.0544 - val_rpn_loss: 0.0588
Epoch 89/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0423 - rpn_loss: 0.0423 - val_loss: 0.0706 - val_rpn_loss: 0.0641
Epoch 90/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0415 - rpn_loss: 0.0415 - val_loss: 0.0555 - val_rpn_loss: 0.0624
Epoch 91/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0433 - rpn_loss: 0.0432 - val_loss: 0.0587 - val_rpn_loss: 0.0575
Epoch 92/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0464 - rpn_loss: 0.0464 - val_loss: 0.0639 - val_rpn_loss: 0.0660
Epoch 93/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0471 - rpn_loss: 0.0471 - val_loss: 0.0614 - val_rpn_loss: 0.0659
Epoch 94/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0431 - rpn_loss: 0.0430 - val_loss: 0.0629 - val_rpn_loss: 0.0611
Epoch 95/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0391 - rpn_loss: 0.0391 - val_loss: 0.0563 - val_rpn_loss: 0.0626
Epoch 96/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0392 - rpn_loss: 0.0392 - val_loss: 0.0619 - val_rpn_loss: 0.0618
Epoch 97/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0396 - rpn_loss: 0.0396 - val_loss: 0.0510 - val_rpn_loss: 0.0589
Epoch 98/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0400 - rpn_loss: 0.0399 - val_loss: 0.0603 - val_rpn_loss: 0.0632
Epoch 99/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0416 - rpn_loss: 0.0415 - val_loss: 0.0704 - val_rpn_loss: 0.0652
Epoch 100/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0443 - rpn_loss: 0.0443 - val_loss: 0.0673 - val_rpn_loss: 0.0648
Epoch 101/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0444 - rpn_loss: 0.0444 - val_loss: 0.0639 - val_rpn_loss: 0.0637
Epoch 102/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0413 - rpn_loss: 0.0413 - val_loss: 0.0595 - val_rpn_loss: 0.0605
Epoch 103/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0399 - rpn_loss: 0.0399 - val_loss: 0.0725 - val_rpn_loss: 0.0614
Epoch 104/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0373 - rpn_loss: 0.0373 - val_loss: 0.0716 - val_rpn_loss: 0.0652
Epoch 105/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0372 - rpn_loss: 0.0372 - val_loss: 0.0555 - val_rpn_loss: 0.0664
Epoch 106/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0370 - rpn_loss: 0.0370 - val_loss: 0.0652 - val_rpn_loss: 0.0638
Epoch 107/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0392 - rpn_loss: 0.0392 - val_loss: 0.0767 - val_rpn_loss: 0.0659
Epoch 108/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0419 - rpn_loss: 0.0419 - val_loss: 0.0520 - val_rpn_loss: 0.0623
Epoch 109/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0423 - rpn_loss: 0.0423 - val_loss: 0.0630 - val_rpn_loss: 0.0635
Epoch 110/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0410 - rpn_loss: 0.0410 - val_loss: 0.0515 - val_rpn_loss: 0.0629
Epoch 111/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0381 - rpn_loss: 0.0381 - val_loss: 0.0692 - val_rpn_loss: 0.0652
Epoch 112/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0368 - rpn_loss: 0.0368 - val_loss: 0.0719 - val_rpn_loss: 0.0636
Epoch 113/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0345 - rpn_loss: 0.0345 - val_loss: 0.0600 - val_rpn_loss: 0.0655
Epoch 114/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0348 - rpn_loss: 0.0348 - val_loss: 0.0625 - val_rpn_loss: 0.0672
Epoch 115/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0378 - rpn_loss: 0.0378 - val_loss: 0.0756 - val_rpn_loss: 0.0648
Epoch 116/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0425 - rpn_loss: 0.0425 - val_loss: 0.0724 - val_rpn_loss: 0.0631
Epoch 117/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0417 - rpn_loss: 0.0417 - val_loss: 0.0771 - val_rpn_loss: 0.0660
Epoch 118/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0378 - rpn_loss: 0.0378 - val_loss: 0.0840 - val_rpn_loss: 0.0682
Epoch 119/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0354 - rpn_loss: 0.0354 - val_loss: 0.0829 - val_rpn_loss: 0.0647
Epoch 120/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0343 - rpn_loss: 0.0343 - val_loss: 0.0559 - val_rpn_loss: 0.0670
Epoch 121/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0350 - rpn_loss: 0.0349 - val_loss: 0.0680 - val_rpn_loss: 0.0669
Epoch 122/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0334 - rpn_loss: 0.0334 - val_loss: 0.0732 - val_rpn_loss: 0.0664
Epoch 123/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0350 - rpn_loss: 0.0350 - val_loss: 0.0539 - val_rpn_loss: 0.0663
Epoch 124/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0398 - rpn_loss: 0.0398 - val_loss: 0.0690 - val_rpn_loss: 0.0696
Epoch 125/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0407 - rpn_loss: 0.0407 - val_loss: 0.0605 - val_rpn_loss: 0.0674
Epoch 126/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0369 - rpn_loss: 0.0369 - val_loss: 0.0681 - val_rpn_loss: 0.0641
Epoch 127/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0329 - rpn_loss: 0.0328 - val_loss: 0.0761 - val_rpn_loss: 0.0682
Epoch 128/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0322 - rpn_loss: 0.0322 - val_loss: 0.0606 - val_rpn_loss: 0.0699
Epoch 129/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0324 - rpn_loss: 0.0323 - val_loss: 0.0621 - val_rpn_loss: 0.0664
Epoch 130/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0321 - rpn_loss: 0.0321 - val_loss: 0.0748 - val_rpn_loss: 0.0694
Epoch 131/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0330 - rpn_loss: 0.0330 - val_loss: 0.0841 - val_rpn_loss: 0.0712
Epoch 132/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0378 - rpn_loss: 0.0378 - val_loss: 0.0774 - val_rpn_loss: 0.0688
Epoch 133/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0388 - rpn_loss: 0.0388 - val_loss: 0.0712 - val_rpn_loss: 0.0700
Epoch 134/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0364 - rpn_loss: 0.0364 - val_loss: 0.0592 - val_rpn_loss: 0.0645
Epoch 135/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0314 - rpn_loss: 0.0314 - val_loss: 0.0651 - val_rpn_loss: 0.0684
Epoch 136/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0297 - rpn_loss: 0.0297 - val_loss: 0.0600 - val_rpn_loss: 0.0732
Epoch 137/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0300 - rpn_loss: 0.0300 - val_loss: 0.0846 - val_rpn_loss: 0.0681
Epoch 138/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0299 - rpn_loss: 0.0299 - val_loss: 0.0760 - val_rpn_loss: 0.0765
Epoch 139/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0320 - rpn_loss: 0.0320 - val_loss: 0.0774 - val_rpn_loss: 0.0689
Epoch 140/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0377 - rpn_loss: 0.0377 - val_loss: 0.0795 - val_rpn_loss: 0.0718
Epoch 141/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0378 - rpn_loss: 0.0378 - val_loss: 0.0616 - val_rpn_loss: 0.0677
Epoch 142/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0345 - rpn_loss: 0.0345 - val_loss: 0.0680 - val_rpn_loss: 0.0675
Epoch 143/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0310 - rpn_loss: 0.0310 - val_loss: 0.0885 - val_rpn_loss: 0.0728
Epoch 144/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0286 - rpn_loss: 0.0286 - val_loss: 0.0623 - val_rpn_loss: 0.0715
Epoch 145/500
309/309 [==============================] - 131s 422ms/step - loss: 0.0278 - rpn_loss: 0.0278 - val_loss: 0.0711 - val_rpn_loss: 0.0780
Epoch 146/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0286 - rpn_loss: 0.0286 - val_loss: 0.0930 - val_rpn_loss: 0.0759
Epoch 147/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0316 - rpn_loss: 0.0316 - val_loss: 0.0623 - val_rpn_loss: 0.0729
Epoch 148/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0359 - rpn_loss: 0.0359 - val_loss: 0.0727 - val_rpn_loss: 0.0705
Epoch 149/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0357 - rpn_loss: 0.0357 - val_loss: 0.0522 - val_rpn_loss: 0.0676
Epoch 150/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0322 - rpn_loss: 0.0322 - val_loss: 0.0777 - val_rpn_loss: 0.0707
Epoch 151/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0296 - rpn_loss: 0.0296 - val_loss: 0.0655 - val_rpn_loss: 0.0715
Epoch 152/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0277 - rpn_loss: 0.0277 - val_loss: 0.0925 - val_rpn_loss: 0.0747
Epoch 153/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0269 - rpn_loss: 0.0269 - val_loss: 0.0767 - val_rpn_loss: 0.0768
Epoch 154/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0262 - rpn_loss: 0.0262 - val_loss: 0.0934 - val_rpn_loss: 0.0775
Epoch 155/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0287 - rpn_loss: 0.0287 - val_loss: 0.0929 - val_rpn_loss: 0.0757
Epoch 156/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0350 - rpn_loss: 0.0350 - val_loss: 0.0647 - val_rpn_loss: 0.0718
Epoch 157/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0354 - rpn_loss: 0.0354 - val_loss: 0.0829 - val_rpn_loss: 0.0746
Epoch 158/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0302 - rpn_loss: 0.0302 - val_loss: 0.0691 - val_rpn_loss: 0.0722
Epoch 159/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0268 - rpn_loss: 0.0268 - val_loss: 0.0647 - val_rpn_loss: 0.0800
Epoch 160/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0261 - rpn_loss: 0.0261 - val_loss: 0.0922 - val_rpn_loss: 0.0772
Epoch 161/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0264 - rpn_loss: 0.0264 - val_loss: 0.0813 - val_rpn_loss: 0.0766
Epoch 162/500
 33/309 [==>...........................] - ETA: 2:17 - loss: 0.0250 - rpn_loss: 0.0250
