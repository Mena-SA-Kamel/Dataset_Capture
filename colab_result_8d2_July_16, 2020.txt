WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:624: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:626: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-16 17:06:13.770102: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2020-07-16 17:06:13.789990: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000144999 Hz
2020-07-16 17:06:13.790418: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2603480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-16 17:06:13.790454: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-16 17:06:13.795076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-16 17:06:14.020260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:14.021000: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x26039c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-16 17:06:14.021043: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-16 17:06:14.021830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:14.022350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-16 17:06:14.022713: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-16 17:06:14.287922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-16 17:06:14.418417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-16 17:06:14.444835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-16 17:06:14.732087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-16 17:06:14.753183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-16 17:06:15.254682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-16 17:06:15.254876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:15.255596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:15.256129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-16 17:06:15.260953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-16 17:06:15.262486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-16 17:06:15.262526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-16 17:06:15.262533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-16 17:06:15.266814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:15.267347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:15.267885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1422: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-16 17:06:42.127209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.127849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-16 17:06:42.127948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-16 17:06:42.127980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-16 17:06:42.127994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-16 17:06:42.128007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-16 17:06:42.128021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-16 17:06:42.128036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-16 17:06:42.128050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-16 17:06:42.128137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.128714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.129230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-16 17:06:42.130210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.130849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-16 17:06:42.130900: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-16 17:06:42.130918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-16 17:06:42.130934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-16 17:06:42.130948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-16 17:06:42.130962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-16 17:06:42.130978: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-16 17:06:42.130993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-16 17:06:42.131047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.131646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.132166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-16 17:06:42.132211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-16 17:06:42.132223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-16 17:06:42.132230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-16 17:06:42.132311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.132919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-16 17:06:42.133419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-16 17:06:42.328240: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-16 17:06:42.328389: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200716T1706/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-16 17:07:51.438863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-16 17:07:57.158726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
309/309 [==============================] - 146s 472ms/step - loss: 0.0995 - rpn_loss: 0.0995 - val_loss: 0.0913 - val_rpn_loss: 0.0926
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0955 - rpn_loss: 0.0955 - val_loss: 0.0925 - val_rpn_loss: 0.0919
Epoch 3/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0953 - rpn_loss: 0.0953 - val_loss: 0.0888 - val_rpn_loss: 0.0931
Epoch 4/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0945 - rpn_loss: 0.0944 - val_loss: 0.0920 - val_rpn_loss: 0.0925
Epoch 5/500
309/309 [==============================] - 89s 287ms/step - loss: 0.0950 - rpn_loss: 0.0950 - val_loss: 0.0867 - val_rpn_loss: 0.0915
Epoch 6/500
309/309 [==============================] - 125s 403ms/step - loss: 0.0948 - rpn_loss: 0.0948 - val_loss: 0.0885 - val_rpn_loss: 0.0912
Epoch 7/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0943 - rpn_loss: 0.0943 - val_loss: 0.0943 - val_rpn_loss: 0.0909
Epoch 8/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0942 - rpn_loss: 0.0942 - val_loss: 0.0907 - val_rpn_loss: 0.0929
Epoch 9/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0946 - rpn_loss: 0.0946 - val_loss: 0.0937 - val_rpn_loss: 0.0927
Epoch 10/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0945 - rpn_loss: 0.0945 - val_loss: 0.0874 - val_rpn_loss: 0.0929
Epoch 11/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0937 - rpn_loss: 0.0937 - val_loss: 0.0922 - val_rpn_loss: 0.0911
Epoch 12/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0944 - rpn_loss: 0.0943 - val_loss: 0.0940 - val_rpn_loss: 0.0906
Epoch 13/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0947 - rpn_loss: 0.0946 - val_loss: 0.0919 - val_rpn_loss: 0.0921
Epoch 14/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0941 - rpn_loss: 0.0941 - val_loss: 0.1007 - val_rpn_loss: 0.0915
Epoch 15/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0939 - rpn_loss: 0.0939 - val_loss: 0.0898 - val_rpn_loss: 0.0916
Epoch 16/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0945 - rpn_loss: 0.0945 - val_loss: 0.0877 - val_rpn_loss: 0.0906
Epoch 17/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0937 - rpn_loss: 0.0937 - val_loss: 0.0907 - val_rpn_loss: 0.0915
Epoch 18/500
309/309 [==============================] - 131s 422ms/step - loss: 0.0935 - rpn_loss: 0.0935 - val_loss: 0.0946 - val_rpn_loss: 0.0915
Epoch 19/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0935 - rpn_loss: 0.0935 - val_loss: 0.0851 - val_rpn_loss: 0.0903
Epoch 20/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0941 - rpn_loss: 0.0941 - val_loss: 0.0991 - val_rpn_loss: 0.0914
Epoch 21/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0935 - rpn_loss: 0.0935 - val_loss: 0.0878 - val_rpn_loss: 0.0920
Epoch 22/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0932 - rpn_loss: 0.0932 - val_loss: 0.0896 - val_rpn_loss: 0.0917
Epoch 23/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0937 - rpn_loss: 0.0937 - val_loss: 0.0883 - val_rpn_loss: 0.0901
Epoch 24/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0934 - rpn_loss: 0.0934 - val_loss: 0.0926 - val_rpn_loss: 0.0910
Epoch 25/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0933 - rpn_loss: 0.0933 - val_loss: 0.0960 - val_rpn_loss: 0.0903
Epoch 26/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0931 - rpn_loss: 0.0931 - val_loss: 0.0906 - val_rpn_loss: 0.0911
Epoch 27/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0924 - rpn_loss: 0.0924 - val_loss: 0.0889 - val_rpn_loss: 0.0909
Epoch 28/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0933 - rpn_loss: 0.0932 - val_loss: 0.0933 - val_rpn_loss: 0.0899
Epoch 29/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0926 - rpn_loss: 0.0925 - val_loss: 0.0865 - val_rpn_loss: 0.0911
Epoch 30/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0925 - rpn_loss: 0.0925 - val_loss: 0.0850 - val_rpn_loss: 0.0895
Epoch 31/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0931 - rpn_loss: 0.0931 - val_loss: 0.0883 - val_rpn_loss: 0.0903
Epoch 32/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0928 - rpn_loss: 0.0928 - val_loss: 0.0905 - val_rpn_loss: 0.0904
Epoch 33/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0925 - rpn_loss: 0.0925 - val_loss: 0.0872 - val_rpn_loss: 0.0904
Epoch 34/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0926 - rpn_loss: 0.0926 - val_loss: 0.0872 - val_rpn_loss: 0.0900
Epoch 35/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0915 - val_rpn_loss: 0.0893
Epoch 36/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0923 - rpn_loss: 0.0923 - val_loss: 0.0906 - val_rpn_loss: 0.0902
Epoch 37/500
309/309 [==============================] - 128s 416ms/step - loss: 0.0926 - rpn_loss: 0.0926 - val_loss: 0.0886 - val_rpn_loss: 0.0910
Epoch 38/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0884 - val_rpn_loss: 0.0898
Epoch 39/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0917 - rpn_loss: 0.0917 - val_loss: 0.0924 - val_rpn_loss: 0.0897
Epoch 40/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0921 - rpn_loss: 0.0921 - val_loss: 0.0882 - val_rpn_loss: 0.0897
Epoch 41/500
309/309 [==============================] - 128s 416ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0885 - val_rpn_loss: 0.0891
Epoch 42/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0916 - rpn_loss: 0.0916 - val_loss: 0.0893 - val_rpn_loss: 0.0892
Epoch 43/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0924 - rpn_loss: 0.0924 - val_loss: 0.0908 - val_rpn_loss: 0.0897
Epoch 44/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0915 - rpn_loss: 0.0915 - val_loss: 0.0836 - val_rpn_loss: 0.0903
Epoch 45/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0911 - rpn_loss: 0.0911 - val_loss: 0.0928 - val_rpn_loss: 0.0883
Epoch 46/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0860 - val_rpn_loss: 0.0885
Epoch 47/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0914 - rpn_loss: 0.0914 - val_loss: 0.0881 - val_rpn_loss: 0.0879
Epoch 48/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0913 - rpn_loss: 0.0913 - val_loss: 0.0935 - val_rpn_loss: 0.0914
Epoch 49/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0914 - rpn_loss: 0.0914 - val_loss: 0.0875 - val_rpn_loss: 0.0892
Epoch 50/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0917 - rpn_loss: 0.0917 - val_loss: 0.0826 - val_rpn_loss: 0.0906
Epoch 51/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0914 - rpn_loss: 0.0914 - val_loss: 0.0885 - val_rpn_loss: 0.0891
Epoch 52/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0906 - rpn_loss: 0.0906 - val_loss: 0.0863 - val_rpn_loss: 0.0881
Epoch 53/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0908 - rpn_loss: 0.0908 - val_loss: 0.0926 - val_rpn_loss: 0.0895
Epoch 54/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0906 - rpn_loss: 0.0906 - val_loss: 0.0842 - val_rpn_loss: 0.0882
Epoch 55/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0909 - rpn_loss: 0.0909 - val_loss: 0.0877 - val_rpn_loss: 0.0882
Epoch 56/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0905 - rpn_loss: 0.0905 - val_loss: 0.0916 - val_rpn_loss: 0.0887
Epoch 57/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0906 - rpn_loss: 0.0906 - val_loss: 0.0854 - val_rpn_loss: 0.0886
Epoch 58/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0902 - rpn_loss: 0.0902 - val_loss: 0.1006 - val_rpn_loss: 0.0875
Epoch 59/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0903 - rpn_loss: 0.0903 - val_loss: 0.0940 - val_rpn_loss: 0.0892
Epoch 60/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0903 - rpn_loss: 0.0903 - val_loss: 0.0829 - val_rpn_loss: 0.0891
Epoch 61/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0893 - rpn_loss: 0.0893 - val_loss: 0.0825 - val_rpn_loss: 0.0876
Epoch 62/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0900 - rpn_loss: 0.0900 - val_loss: 0.0868 - val_rpn_loss: 0.0861
Epoch 63/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0897 - rpn_loss: 0.0897 - val_loss: 0.0913 - val_rpn_loss: 0.0880
Epoch 64/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0891 - rpn_loss: 0.0891 - val_loss: 0.0865 - val_rpn_loss: 0.0882
Epoch 65/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0895 - rpn_loss: 0.0894 - val_loss: 0.0829 - val_rpn_loss: 0.0868
Epoch 66/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0898 - rpn_loss: 0.0897 - val_loss: 0.0837 - val_rpn_loss: 0.0867
Epoch 67/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0888 - rpn_loss: 0.0888 - val_loss: 0.0891 - val_rpn_loss: 0.0876
Epoch 68/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0885 - rpn_loss: 0.0885 - val_loss: 0.0921 - val_rpn_loss: 0.0876
Epoch 69/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0884 - rpn_loss: 0.0884 - val_loss: 0.0860 - val_rpn_loss: 0.0863
Epoch 70/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0880 - rpn_loss: 0.0879 - val_loss: 0.0882 - val_rpn_loss: 0.0865
Epoch 71/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0876 - rpn_loss: 0.0876 - val_loss: 0.0845 - val_rpn_loss: 0.0864
Epoch 72/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0869 - rpn_loss: 0.0869 - val_loss: 0.0885 - val_rpn_loss: 0.0860
Epoch 73/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0869 - rpn_loss: 0.0869 - val_loss: 0.0891 - val_rpn_loss: 0.0847
Epoch 74/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0865 - rpn_loss: 0.0865 - val_loss: 0.0806 - val_rpn_loss: 0.0852
Epoch 75/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0851 - rpn_loss: 0.0851 - val_loss: 0.0774 - val_rpn_loss: 0.0837
Epoch 76/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0850 - rpn_loss: 0.0850 - val_loss: 0.0781 - val_rpn_loss: 0.0843
Epoch 77/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0851 - rpn_loss: 0.0851 - val_loss: 0.0957 - val_rpn_loss: 0.0842
Epoch 78/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0843 - rpn_loss: 0.0843 - val_loss: 0.0824 - val_rpn_loss: 0.0839
Epoch 79/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0839 - rpn_loss: 0.0839 - val_loss: 0.0869 - val_rpn_loss: 0.0828
Epoch 80/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0830 - rpn_loss: 0.0830 - val_loss: 0.0770 - val_rpn_loss: 0.0823
Epoch 81/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0823 - rpn_loss: 0.0823 - val_loss: 0.0813 - val_rpn_loss: 0.0823
Epoch 82/500
309/309 [==============================] - 138s 445ms/step - loss: 0.0820 - rpn_loss: 0.0820 - val_loss: 0.0784 - val_rpn_loss: 0.0815
Epoch 83/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0824 - rpn_loss: 0.0824 - val_loss: 0.0785 - val_rpn_loss: 0.0812
Epoch 84/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0808 - rpn_loss: 0.0807 - val_loss: 0.0864 - val_rpn_loss: 0.0795
Epoch 85/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0804 - rpn_loss: 0.0804 - val_loss: 0.0881 - val_rpn_loss: 0.0806
Epoch 86/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0797 - rpn_loss: 0.0797 - val_loss: 0.0727 - val_rpn_loss: 0.0801
Epoch 87/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0785 - rpn_loss: 0.0785 - val_loss: 0.0755 - val_rpn_loss: 0.0800
Epoch 88/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0801 - rpn_loss: 0.0801 - val_loss: 0.0809 - val_rpn_loss: 0.0801
Epoch 89/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0786 - rpn_loss: 0.0786 - val_loss: 0.0713 - val_rpn_loss: 0.0783
Epoch 90/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0780 - rpn_loss: 0.0780 - val_loss: 0.0796 - val_rpn_loss: 0.0781
Epoch 91/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0774 - rpn_loss: 0.0774 - val_loss: 0.0811 - val_rpn_loss: 0.0798
Epoch 92/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0766 - rpn_loss: 0.0766 - val_loss: 0.0855 - val_rpn_loss: 0.0788
Epoch 93/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0758 - rpn_loss: 0.0758 - val_loss: 0.0758 - val_rpn_loss: 0.0766
Epoch 94/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0765 - rpn_loss: 0.0765 - val_loss: 0.0749 - val_rpn_loss: 0.0761
Epoch 95/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0749 - rpn_loss: 0.0749 - val_loss: 0.0782 - val_rpn_loss: 0.0743
Epoch 96/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0746 - rpn_loss: 0.0746 - val_loss: 0.0830 - val_rpn_loss: 0.0771
Epoch 97/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0749 - rpn_loss: 0.0749 - val_loss: 0.0785 - val_rpn_loss: 0.0772
Epoch 98/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0744 - rpn_loss: 0.0744 - val_loss: 0.0831 - val_rpn_loss: 0.0756
Epoch 99/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0738 - rpn_loss: 0.0738 - val_loss: 0.0755 - val_rpn_loss: 0.0742
Epoch 100/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0729 - rpn_loss: 0.0729 - val_loss: 0.0745 - val_rpn_loss: 0.0744
Epoch 101/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0731 - rpn_loss: 0.0730 - val_loss: 0.0795 - val_rpn_loss: 0.0743
Epoch 102/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0734 - rpn_loss: 0.0734 - val_loss: 0.0736 - val_rpn_loss: 0.0753
Epoch 103/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0728 - rpn_loss: 0.0728 - val_loss: 0.0663 - val_rpn_loss: 0.0742
Epoch 104/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0716 - rpn_loss: 0.0716 - val_loss: 0.0721 - val_rpn_loss: 0.0719
Epoch 105/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0716 - rpn_loss: 0.0716 - val_loss: 0.0638 - val_rpn_loss: 0.0731
Epoch 106/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0723 - rpn_loss: 0.0723 - val_loss: 0.0827 - val_rpn_loss: 0.0724
Epoch 107/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0724 - rpn_loss: 0.0724 - val_loss: 0.0742 - val_rpn_loss: 0.0732
Epoch 108/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0715 - rpn_loss: 0.0714 - val_loss: 0.0676 - val_rpn_loss: 0.0733
Epoch 109/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0711 - rpn_loss: 0.0711 - val_loss: 0.0689 - val_rpn_loss: 0.0727
Epoch 110/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0709 - rpn_loss: 0.0709 - val_loss: 0.0765 - val_rpn_loss: 0.0728
Epoch 111/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0695 - rpn_loss: 0.0695 - val_loss: 0.0712 - val_rpn_loss: 0.0713
Epoch 112/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0701 - rpn_loss: 0.0700 - val_loss: 0.0693 - val_rpn_loss: 0.0718
Epoch 113/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0699 - rpn_loss: 0.0699 - val_loss: 0.0748 - val_rpn_loss: 0.0733
Epoch 114/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0693 - rpn_loss: 0.0693 - val_loss: 0.0737 - val_rpn_loss: 0.0714
Epoch 115/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0692 - rpn_loss: 0.0692 - val_loss: 0.0766 - val_rpn_loss: 0.0724
Epoch 116/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0682 - rpn_loss: 0.0682 - val_loss: 0.0721 - val_rpn_loss: 0.0707
Epoch 117/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0680 - rpn_loss: 0.0680 - val_loss: 0.0638 - val_rpn_loss: 0.0702
Epoch 118/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0687 - rpn_loss: 0.0687 - val_loss: 0.0686 - val_rpn_loss: 0.0683
Epoch 119/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0680 - rpn_loss: 0.0680 - val_loss: 0.0686 - val_rpn_loss: 0.0710
Epoch 120/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0679 - rpn_loss: 0.0679 - val_loss: 0.0667 - val_rpn_loss: 0.0696
Epoch 121/500
309/309 [==============================] - 128s 416ms/step - loss: 0.0674 - rpn_loss: 0.0674 - val_loss: 0.0705 - val_rpn_loss: 0.0707
Epoch 122/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0666 - rpn_loss: 0.0666 - val_loss: 0.0740 - val_rpn_loss: 0.0717
Epoch 123/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0673 - rpn_loss: 0.0673 - val_loss: 0.0740 - val_rpn_loss: 0.0700
Epoch 124/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0667 - rpn_loss: 0.0667 - val_loss: 0.0745 - val_rpn_loss: 0.0700
Epoch 125/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0673 - rpn_loss: 0.0673 - val_loss: 0.0722 - val_rpn_loss: 0.0684
Epoch 126/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0656 - rpn_loss: 0.0655 - val_loss: 0.0720 - val_rpn_loss: 0.0697
Epoch 127/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0660 - rpn_loss: 0.0660 - val_loss: 0.0701 - val_rpn_loss: 0.0694
Epoch 128/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0667 - rpn_loss: 0.0666 - val_loss: 0.0712 - val_rpn_loss: 0.0669
Epoch 129/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0657 - rpn_loss: 0.0656 - val_loss: 0.0649 - val_rpn_loss: 0.0691
Epoch 130/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0658 - rpn_loss: 0.0658 - val_loss: 0.0696 - val_rpn_loss: 0.0689
Epoch 131/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0657 - rpn_loss: 0.0657 - val_loss: 0.0723 - val_rpn_loss: 0.0709
Epoch 132/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0651 - rpn_loss: 0.0651 - val_loss: 0.0634 - val_rpn_loss: 0.0685
Epoch 133/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0647 - rpn_loss: 0.0647 - val_loss: 0.0707 - val_rpn_loss: 0.0670
Epoch 134/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0651 - rpn_loss: 0.0651 - val_loss: 0.0699 - val_rpn_loss: 0.0679
Epoch 135/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0649 - rpn_loss: 0.0649 - val_loss: 0.0689 - val_rpn_loss: 0.0707
Epoch 136/500
309/309 [==============================] - 128s 416ms/step - loss: 0.0645 - rpn_loss: 0.0645 - val_loss: 0.0677 - val_rpn_loss: 0.0701
Epoch 137/500
309/309 [==============================] - 127s 412ms/step - loss: 0.0638 - rpn_loss: 0.0638 - val_loss: 0.0768 - val_rpn_loss: 0.0697
Epoch 138/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0640 - rpn_loss: 0.0640 - val_loss: 0.0693 - val_rpn_loss: 0.0695
Epoch 139/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0641 - rpn_loss: 0.0641 - val_loss: 0.0744 - val_rpn_loss: 0.0662
Epoch 140/500
309/309 [==============================] - 128s 413ms/step - loss: 0.0640 - rpn_loss: 0.0640 - val_loss: 0.0687 - val_rpn_loss: 0.0674
Epoch 141/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0634 - rpn_loss: 0.0634 - val_loss: 0.0731 - val_rpn_loss: 0.0679
Epoch 142/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0637 - rpn_loss: 0.0637 - val_loss: 0.0667 - val_rpn_loss: 0.0686
Epoch 143/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0633 - rpn_loss: 0.0633 - val_loss: 0.0633 - val_rpn_loss: 0.0671
Epoch 144/500
309/309 [==============================] - 128s 416ms/step - loss: 0.0632 - rpn_loss: 0.0632 - val_loss: 0.0649 - val_rpn_loss: 0.0665
Epoch 145/500
309/309 [==============================] - 128s 413ms/step - loss: 0.0631 - rpn_loss: 0.0631 - val_loss: 0.0649 - val_rpn_loss: 0.0679
Epoch 146/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0624 - rpn_loss: 0.0624 - val_loss: 0.0738 - val_rpn_loss: 0.0692
Epoch 147/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0631 - rpn_loss: 0.0631 - val_loss: 0.0695 - val_rpn_loss: 0.0679
Epoch 148/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0628 - rpn_loss: 0.0628 - val_loss: 0.0662 - val_rpn_loss: 0.0662
Epoch 149/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0619 - rpn_loss: 0.0619 - val_loss: 0.0678 - val_rpn_loss: 0.0667
Epoch 150/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0623 - rpn_loss: 0.0623 - val_loss: 0.0708 - val_rpn_loss: 0.0676
Epoch 151/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0621 - rpn_loss: 0.0620 - val_loss: 0.0693 - val_rpn_loss: 0.0669
Epoch 152/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0619 - rpn_loss: 0.0619 - val_loss: 0.0599 - val_rpn_loss: 0.0674
Epoch 153/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0610 - rpn_loss: 0.0610 - val_loss: 0.0619 - val_rpn_loss: 0.0653
Epoch 154/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0614 - rpn_loss: 0.0614 - val_loss: 0.0651 - val_rpn_loss: 0.0666
Epoch 155/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0620 - rpn_loss: 0.0620 - val_loss: 0.0618 - val_rpn_loss: 0.0653
Epoch 156/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0611 - rpn_loss: 0.0611 - val_loss: 0.0652 - val_rpn_loss: 0.0664
Epoch 157/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0619 - rpn_loss: 0.0619 - val_loss: 0.0671 - val_rpn_loss: 0.0679
Epoch 158/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0610 - rpn_loss: 0.0610 - val_loss: 0.0701 - val_rpn_loss: 0.0671
Epoch 159/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0610 - rpn_loss: 0.0610 - val_loss: 0.0728 - val_rpn_loss: 0.0659
Epoch 160/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0611 - rpn_loss: 0.0611 - val_loss: 0.0758 - val_rpn_loss: 0.0665
Epoch 161/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0606 - rpn_loss: 0.0606 - val_loss: 0.0681 - val_rpn_loss: 0.0667
Epoch 162/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0600 - rpn_loss: 0.0600 - val_loss: 0.0681 - val_rpn_loss: 0.0673
Epoch 163/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0602 - rpn_loss: 0.0602 - val_loss: 0.0673 - val_rpn_loss: 0.0660
Epoch 164/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0603 - rpn_loss: 0.0603 - val_loss: 0.0768 - val_rpn_loss: 0.0655
Epoch 165/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0595 - rpn_loss: 0.0595 - val_loss: 0.0670 - val_rpn_loss: 0.0665
Epoch 166/500
309/309 [==============================] - 130s 419ms/step - loss: 0.0599 - rpn_loss: 0.0599 - val_loss: 0.0631 - val_rpn_loss: 0.0662
Epoch 167/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0593 - rpn_loss: 0.0593 - val_loss: 0.0749 - val_rpn_loss: 0.0675
Epoch 168/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0587 - rpn_loss: 0.0587 - val_loss: 0.0707 - val_rpn_loss: 0.0676
Epoch 169/500
309/309 [==============================] - 128s 413ms/step - loss: 0.0596 - rpn_loss: 0.0596 - val_loss: 0.0728 - val_rpn_loss: 0.0665
Epoch 170/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0593 - rpn_loss: 0.0593 - val_loss: 0.0709 - val_rpn_loss: 0.0653
Epoch 171/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0591 - rpn_loss: 0.0591 - val_loss: 0.0581 - val_rpn_loss: 0.0666
Epoch 172/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0581 - rpn_loss: 0.0581 - val_loss: 0.0627 - val_rpn_loss: 0.0653
Epoch 173/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0586 - rpn_loss: 0.0586 - val_loss: 0.0620 - val_rpn_loss: 0.0643
Epoch 174/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0586 - rpn_loss: 0.0586 - val_loss: 0.0728 - val_rpn_loss: 0.0651
Epoch 175/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0582 - rpn_loss: 0.0582 - val_loss: 0.0585 - val_rpn_loss: 0.0638
Epoch 176/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0570 - rpn_loss: 0.0570 - val_loss: 0.0616 - val_rpn_loss: 0.0683
Epoch 177/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0582 - rpn_loss: 0.0582 - val_loss: 0.0673 - val_rpn_loss: 0.0672
Epoch 178/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0578 - rpn_loss: 0.0578 - val_loss: 0.0506 - val_rpn_loss: 0.0633
Epoch 179/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0591 - rpn_loss: 0.0591 - val_loss: 0.0541 - val_rpn_loss: 0.0648
Epoch 180/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0574 - rpn_loss: 0.0574 - val_loss: 0.0592 - val_rpn_loss: 0.0660
Epoch 181/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0566 - rpn_loss: 0.0566 - val_loss: 0.0660 - val_rpn_loss: 0.0656
Epoch 182/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0581 - rpn_loss: 0.0581 - val_loss: 0.0718 - val_rpn_loss: 0.0667
Epoch 183/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0574 - rpn_loss: 0.0574 - val_loss: 0.0670 - val_rpn_loss: 0.0660
Epoch 184/500
309/309 [==============================] - 128s 416ms/step - loss: 0.0572 - rpn_loss: 0.0572 - val_loss: 0.0581 - val_rpn_loss: 0.0652
Epoch 185/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0570 - rpn_loss: 0.0570 - val_loss: 0.0520 - val_rpn_loss: 0.0616
Epoch 186/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0560 - rpn_loss: 0.0560 - val_loss: 0.0587 - val_rpn_loss: 0.0629
Epoch 187/500
309/309 [==============================] - 129s 418ms/step - loss: 0.0566 - rpn_loss: 0.0565 - val_loss: 0.0654 - val_rpn_loss: 0.0648
Epoch 188/500
309/309 [==============================] - 129s 416ms/step - loss: 0.0569 - rpn_loss: 0.0569 - val_loss: 0.0591 - val_rpn_loss: 0.0657
Epoch 189/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0559 - rpn_loss: 0.0559 - val_loss: 0.0604 - val_rpn_loss: 0.0644
Epoch 190/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0558 - rpn_loss: 0.0558 - val_loss: 0.0619 - val_rpn_loss: 0.0636
Epoch 191/500
309/309 [==============================] - 129s 417ms/step - loss: 0.0565 - rpn_loss: 0.0565 - val_loss: 0.0575 - val_rpn_loss: 0.0643
Epoch 192/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
########################################################
Epoch 193/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 194/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 195/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 196/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 197/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 198/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 199/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
Epoch 200/500
309/309 [==============================] - 128s 415ms/step - loss: 0.0561 - rpn_loss: 0.0561 - val_loss: 0.0810 - val_rpn_loss: 0.0644
########################################################
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:624: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:626: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-17 11:28:11.879539: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2020-07-17 11:28:11.884750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000144999 Hz
2020-07-17 11:28:11.885051: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x300aa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-17 11:28:11.885087: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-17 11:28:11.890267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-17 11:28:11.999064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:12.000005: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x300abc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-17 11:28:12.000041: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-17 11:28:12.000218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:12.000771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-17 11:28:12.001127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 11:28:12.020698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-17 11:28:12.028378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-17 11:28:12.039001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-17 11:28:12.052074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-17 11:28:12.058030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-17 11:28:12.075019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 11:28:12.075229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:12.075834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:12.076316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-17 11:28:12.076380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 11:28:12.077584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-17 11:28:12.077612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-17 11:28:12.077623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-17 11:28:12.077741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:12.078286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:12.078814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1422: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-17 11:28:30.964930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.965651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-17 11:28:30.965750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 11:28:30.965778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-17 11:28:30.965800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-17 11:28:30.965820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-17 11:28:30.965840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-17 11:28:30.965861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-17 11:28:30.965881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 11:28:30.965965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.966532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.967018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-17 11:28:30.967687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.968196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-17 11:28:30.968251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 11:28:30.968279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-17 11:28:30.968301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-17 11:28:30.968323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-17 11:28:30.968344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-17 11:28:30.968361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-17 11:28:30.968380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 11:28:30.968445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.968989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.969484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-17 11:28:30.969538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-17 11:28:30.969551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-17 11:28:30.969561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-17 11:28:30.969670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.970203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 11:28:30.970723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-17 11:28:31.167419: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-17 11:28:31.167735: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.02

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200717T1128/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-17 11:29:36.259746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 11:29:40.504688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
309/309 [==============================] - 146s 471ms/step - loss: 0.0780 - rpn_loss: 0.0780 - val_loss: 0.0774 - val_rpn_loss: 0.0768
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0704 - rpn_loss: 0.0704 - val_loss: 0.0720 - val_rpn_loss: 0.0715
Epoch 3/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0675 - rpn_loss: 0.0675 - val_loss: 0.0777 - val_rpn_loss: 0.0683
Epoch 4/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0657 - rpn_loss: 0.0657 - val_loss: 0.0717 - val_rpn_loss: 0.0718
Epoch 5/500
309/309 [==============================] - 89s 287ms/step - loss: 0.0646 - rpn_loss: 0.0646 - val_loss: 0.0622 - val_rpn_loss: 0.0648
Epoch 6/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0619 - rpn_loss: 0.0618 - val_loss: 0.0687 - val_rpn_loss: 0.0655
Epoch 7/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0625 - rpn_loss: 0.0625 - val_loss: 0.0629 - val_rpn_loss: 0.0669
Epoch 8/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0642 - rpn_loss: 0.0642 - val_loss: 0.0653 - val_rpn_loss: 0.0644
Epoch 9/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0605 - rpn_loss: 0.0605 - val_loss: 0.0578 - val_rpn_loss: 0.0653
Epoch 10/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0589 - rpn_loss: 0.0589 - val_loss: 0.0688 - val_rpn_loss: 0.0625
Epoch 11/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0589 - rpn_loss: 0.0589 - val_loss: 0.0715 - val_rpn_loss: 0.0683
Epoch 12/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0579 - rpn_loss: 0.0579 - val_loss: 0.0639 - val_rpn_loss: 0.0648
Epoch 13/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0571 - rpn_loss: 0.0571 - val_loss: 0.0732 - val_rpn_loss: 0.0651
Epoch 14/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0574 - rpn_loss: 0.0574 - val_loss: 0.0645 - val_rpn_loss: 0.0636
Epoch 15/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0554 - rpn_loss: 0.0554 - val_loss: 0.0642 - val_rpn_loss: 0.0602
Epoch 16/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0550 - rpn_loss: 0.0550 - val_loss: 0.0670 - val_rpn_loss: 0.0682
Epoch 17/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0559 - rpn_loss: 0.0559 - val_loss: 0.0661 - val_rpn_loss: 0.0633
Epoch 18/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0550 - rpn_loss: 0.0550 - val_loss: 0.0628 - val_rpn_loss: 0.0637
Epoch 19/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0537 - rpn_loss: 0.0537 - val_loss: 0.0627 - val_rpn_loss: 0.0631
Epoch 20/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0541 - rpn_loss: 0.0541 - val_loss: 0.0575 - val_rpn_loss: 0.0616
Epoch 21/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0544 - rpn_loss: 0.0544 - val_loss: 0.0666 - val_rpn_loss: 0.0613
Epoch 22/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0553 - rpn_loss: 0.0553 - val_loss: 0.0457 - val_rpn_loss: 0.0615
Epoch 23/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0528 - rpn_loss: 0.0528 - val_loss: 0.0577 - val_rpn_loss: 0.0621
Epoch 24/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0527 - rpn_loss: 0.0527 - val_loss: 0.0747 - val_rpn_loss: 0.0602
Epoch 25/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0513 - rpn_loss: 0.0513 - val_loss: 0.0545 - val_rpn_loss: 0.0592
Epoch 26/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0614 - val_rpn_loss: 0.0593
Epoch 27/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0515 - rpn_loss: 0.0515 - val_loss: 0.0613 - val_rpn_loss: 0.0593
Epoch 28/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0500 - rpn_loss: 0.0500 - val_loss: 0.0561 - val_rpn_loss: 0.0595
Epoch 29/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0611 - val_rpn_loss: 0.0576
Epoch 30/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0491 - rpn_loss: 0.0491 - val_loss: 0.0631 - val_rpn_loss: 0.0631
Epoch 31/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0633 - val_rpn_loss: 0.0615
Epoch 32/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0478 - rpn_loss: 0.0478 - val_loss: 0.0668 - val_rpn_loss: 0.0605
Epoch 33/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0474 - rpn_loss: 0.0474 - val_loss: 0.0604 - val_rpn_loss: 0.0618
Epoch 34/500
309/309 [==============================] - 145s 470ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0547 - val_rpn_loss: 0.0606
Epoch 35/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0522 - val_rpn_loss: 0.0595
Epoch 36/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0480 - rpn_loss: 0.0480 - val_loss: 0.0548 - val_rpn_loss: 0.0597
Epoch 37/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0469 - rpn_loss: 0.0469 - val_loss: 0.0605 - val_rpn_loss: 0.0627
Epoch 38/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0459 - rpn_loss: 0.0459 - val_loss: 0.0653 - val_rpn_loss: 0.0590
Epoch 39/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0472 - rpn_loss: 0.0472 - val_loss: 0.0653 - val_rpn_loss: 0.0589
Epoch 40/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0472 - rpn_loss: 0.0472 - val_loss: 0.0653 - val_rpn_loss: 0.0597
########################################################

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:624: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:626: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-17 15:09:49.797791: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2020-07-17 15:09:49.803035: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz
2020-07-17 15:09:49.803333: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e8ca00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-17 15:09:49.803363: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-17 15:09:49.805375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-17 15:09:49.913497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:49.914386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e8cbc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-17 15:09:49.914416: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-17 15:09:49.914566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:49.915093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-17 15:09:49.915398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 15:09:49.916910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-17 15:09:49.918716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-17 15:09:49.919043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-17 15:09:49.920492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-17 15:09:49.921223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-17 15:09:49.924602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 15:09:49.924725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:49.925276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:49.925760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-17 15:09:49.925823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 15:09:49.927113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-17 15:09:49.927138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-17 15:09:49.927146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-17 15:09:49.927258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:49.927801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:49.928310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1422: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-17 15:09:54.606787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.607346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-17 15:09:54.607458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 15:09:54.607485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-17 15:09:54.607512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-17 15:09:54.607547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-17 15:09:54.607569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-17 15:09:54.607591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-17 15:09:54.607612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 15:09:54.607710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.608259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.608756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-17 15:09:54.609371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.609897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-17 15:09:54.609952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-17 15:09:54.609980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-17 15:09:54.610002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-17 15:09:54.610026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-17 15:09:54.610048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-17 15:09:54.610068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-17 15:09:54.610088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 15:09:54.610153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.610703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.611195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-17 15:09:54.611239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-17 15:09:54.611252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-17 15:09:54.611265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-17 15:09:54.611362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.611921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-17 15:09:54.612410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-17 15:09:54.798887: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-17 15:09:54.799058: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200717T1509/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-17 15:10:56.972642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-17 15:11:01.411234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
309/309 [==============================] - 148s 480ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0496 - val_rpn_loss: 0.0576
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0509 - rpn_loss: 0.0509 - val_loss: 0.0459 - val_rpn_loss: 0.0557
Epoch 3/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0579 - val_rpn_loss: 0.0590
Epoch 4/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0507 - rpn_loss: 0.0506 - val_loss: 0.0477 - val_rpn_loss: 0.0564
Epoch 5/500
309/309 [==============================] - 90s 290ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0532 - val_rpn_loss: 0.0600
Epoch 6/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0483 - rpn_loss: 0.0483 - val_loss: 0.0613 - val_rpn_loss: 0.0557
Epoch 7/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0499 - rpn_loss: 0.0499 - val_loss: 0.0626 - val_rpn_loss: 0.0574
Epoch 8/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0610 - val_rpn_loss: 0.0568
Epoch 9/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0482 - rpn_loss: 0.0482 - val_loss: 0.0635 - val_rpn_loss: 0.0572
Epoch 10/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0485 - rpn_loss: 0.0484 - val_loss: 0.0606 - val_rpn_loss: 0.0580
Epoch 11/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0478 - rpn_loss: 0.0478 - val_loss: 0.0554 - val_rpn_loss: 0.0599
Epoch 12/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0484 - rpn_loss: 0.0483 - val_loss: 0.0445 - val_rpn_loss: 0.0543
Epoch 13/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0469 - rpn_loss: 0.0469 - val_loss: 0.0532 - val_rpn_loss: 0.0572
Epoch 14/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0462 - rpn_loss: 0.0461 - val_loss: 0.0518 - val_rpn_loss: 0.0571
Epoch 15/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0469 - rpn_loss: 0.0469 - val_loss: 0.0603 - val_rpn_loss: 0.0580
Epoch 16/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0470 - rpn_loss: 0.0469 - val_loss: 0.0630 - val_rpn_loss: 0.0583
Epoch 17/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0471 - rpn_loss: 0.0471 - val_loss: 0.0551 - val_rpn_loss: 0.0553
Epoch 18/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0469 - rpn_loss: 0.0469 - val_loss: 0.0639 - val_rpn_loss: 0.0566
Epoch 19/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0460 - rpn_loss: 0.0460 - val_loss: 0.0543 - val_rpn_loss: 0.0570
Epoch 20/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0459 - rpn_loss: 0.0459 - val_loss: 0.0551 - val_rpn_loss: 0.0588
Epoch 21/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0470 - rpn_loss: 0.0470 - val_loss: 0.0510 - val_rpn_loss: 0.0561
Epoch 22/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0463 - rpn_loss: 0.0463 - val_loss: 0.0492 - val_rpn_loss: 0.0582
Epoch 23/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0457 - rpn_loss: 0.0457 - val_loss: 0.0737 - val_rpn_loss: 0.0591
Epoch 24/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0460 - rpn_loss: 0.0460 - val_loss: 0.0688 - val_rpn_loss: 0.0589
Epoch 25/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0459 - rpn_loss: 0.0459 - val_loss: 0.0506 - val_rpn_loss: 0.0576
Epoch 26/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0450 - rpn_loss: 0.0450 - val_loss: 0.0562 - val_rpn_loss: 0.0547
Epoch 27/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0461 - rpn_loss: 0.0461 - val_loss: 0.0560 - val_rpn_loss: 0.0574
Epoch 28/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0454 - rpn_loss: 0.0453 - val_loss: 0.0607 - val_rpn_loss: 0.0533
Epoch 29/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0445 - rpn_loss: 0.0445 - val_loss: 0.0706 - val_rpn_loss: 0.0589
Epoch 30/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0450 - rpn_loss: 0.0450 - val_loss: 0.0663 - val_rpn_loss: 0.0608
Epoch 31/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0457 - rpn_loss: 0.0456 - val_loss: 0.0606 - val_rpn_loss: 0.0571
Epoch 32/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0451 - rpn_loss: 0.0451 - val_loss: 0.0491 - val_rpn_loss: 0.0591
Epoch 33/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0446 - rpn_loss: 0.0446 - val_loss: 0.0527 - val_rpn_loss: 0.0568
Epoch 34/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0450 - rpn_loss: 0.0450 - val_loss: 0.0653 - val_rpn_loss: 0.0615
Epoch 35/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0446 - rpn_loss: 0.0446 - val_loss: 0.0635 - val_rpn_loss: 0.0564
Epoch 36/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0444 - rpn_loss: 0.0444 - val_loss: 0.0575 - val_rpn_loss: 0.0558
Epoch 37/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0442 - rpn_loss: 0.0442 - val_loss: 0.0532 - val_rpn_loss: 0.0589
Epoch 38/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0441 - rpn_loss: 0.0441 - val_loss: 0.0630 - val_rpn_loss: 0.0578
Epoch 39/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0445 - rpn_loss: 0.0445 - val_loss: 0.0668 - val_rpn_loss: 0.0580
Epoch 40/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0444 - rpn_loss: 0.0444 - val_loss: 0.0664 - val_rpn_loss: 0.0585
Epoch 41/500
309/309 [==============================] - 130s 421ms/step - loss: 0.0447 - rpn_loss: 0.0447 - val_loss: 0.0617 - val_rpn_loss: 0.0580
Epoch 42/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0432 - rpn_loss: 0.0432 - val_loss: 0.0552 - val_rpn_loss: 0.0591
Epoch 43/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0443 - rpn_loss: 0.0443 - val_loss: 0.0555 - val_rpn_loss: 0.0560
Epoch 44/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0439 - rpn_loss: 0.0439 - val_loss: 0.0621 - val_rpn_loss: 0.0601
Epoch 45/500
309/309 [==============================] - 131s 422ms/step - loss: 0.0431 - rpn_loss: 0.0431 - val_loss: 0.0645 - val_rpn_loss: 0.0577
Epoch 46/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0431 - rpn_loss: 0.0431 - val_loss: 0.0611 - val_rpn_loss: 0.0584
Epoch 47/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0432 - rpn_loss: 0.0432 - val_loss: 0.0654 - val_rpn_loss: 0.0584
Epoch 48/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0432 - rpn_loss: 0.0432 - val_loss: 0.0673 - val_rpn_loss: 0.0584
Epoch 49/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0440 - rpn_loss: 0.0439 - val_loss: 0.0533 - val_rpn_loss: 0.0600
Epoch 50/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0426 - rpn_loss: 0.0425 - val_loss: 0.0461 - val_rpn_loss: 0.0580
Epoch 51/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0431 - rpn_loss: 0.0431 - val_loss: 0.0709 - val_rpn_loss: 0.0590
Epoch 52/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0429 - rpn_loss: 0.0429 - val_loss: 0.0592 - val_rpn_loss: 0.0594
Epoch 53/500
136/309 [============>.................] - ETA: 1:19 - loss: 0.0432 - rpn_loss: 0.0432
