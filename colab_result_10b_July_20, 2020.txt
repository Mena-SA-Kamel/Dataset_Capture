WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:624: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:626: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-20 20:38:00.049999: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-07-20 20:38:00.050258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3269480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-20 20:38:00.050289: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-20 20:38:00.052019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-20 20:38:00.159787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.160555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32699c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-20 20:38:00.160619: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-20 20:38:00.160873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.161395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 20:38:00.161694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:00.163189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 20:38:00.164646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 20:38:00.164975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 20:38:00.166397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 20:38:00.167102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 20:38:00.170078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:38:00.170198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.171007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.171531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 20:38:00.171593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:00.172925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-20 20:38:00.172957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-20 20:38:00.172965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-20 20:38:00.173081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.173598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:00.174149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1423: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-20 20:38:05.497611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.498240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 20:38:05.498327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:05.498347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 20:38:05.498360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 20:38:05.498375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 20:38:05.498389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 20:38:05.498404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 20:38:05.498418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:38:05.498487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.499045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.499537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 20:38:05.500229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.500775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-20 20:38:05.500836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-20 20:38:05.500892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-20 20:38:05.500902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-20 20:38:05.500915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-20 20:38:05.500928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-20 20:38:05.500940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-20 20:38:05.500953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:38:05.501023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.501552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.502070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-20 20:38:05.502125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-20 20:38:05.502137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-20 20:38:05.502144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-20 20:38:05.502235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.502785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-20 20:38:05.503266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-20 20:38:05.696126: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-20 20:38:05.696306: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200720T2038/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-20 20:39:11.907759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-20 20:39:16.569070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
  2/309 [..............................] - ETA: 2:00:57 - loss: 0.2374 - rpn_loss: 0.2374/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.496090). Check your callbacks.
  % (hook_name, delta_t_median), RuntimeWarning)
309/309 [==============================] - 151s 489ms/step - loss: 0.1032 - rpn_loss: 0.1032 - val_loss: 0.0920 - val_rpn_loss: 0.0944
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0952 - rpn_loss: 0.0952 - val_loss: 0.0906 - val_rpn_loss: 0.0923
Epoch 3/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0954 - rpn_loss: 0.0954 - val_loss: 0.0877 - val_rpn_loss: 0.0921
Epoch 4/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0934 - rpn_loss: 0.0933 - val_loss: 0.0938 - val_rpn_loss: 0.0908
Epoch 5/500
309/309 [==============================] - 92s 297ms/step - loss: 0.0913 - rpn_loss: 0.0913 - val_loss: 0.0906 - val_rpn_loss: 0.0909
Epoch 6/500
309/309 [==============================] - 131s 424ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0944 - val_rpn_loss: 0.0901
Epoch 7/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0922 - rpn_loss: 0.0922 - val_loss: 0.0900 - val_rpn_loss: 0.0901
Epoch 8/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0919 - rpn_loss: 0.0919 - val_loss: 0.0873 - val_rpn_loss: 0.0891
Epoch 9/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0915 - rpn_loss: 0.0915 - val_loss: 0.0916 - val_rpn_loss: 0.0887
Epoch 10/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0913 - rpn_loss: 0.0913 - val_loss: 0.0900 - val_rpn_loss: 0.0912
Epoch 11/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0909 - rpn_loss: 0.0909 - val_loss: 0.0895 - val_rpn_loss: 0.0891
Epoch 12/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0912 - rpn_loss: 0.0911 - val_loss: 0.0851 - val_rpn_loss: 0.0885
Epoch 13/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0891 - rpn_loss: 0.0890 - val_loss: 0.0843 - val_rpn_loss: 0.0866
Epoch 14/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0875 - rpn_loss: 0.0875 - val_loss: 0.0795 - val_rpn_loss: 0.0840
Epoch 15/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0839 - rpn_loss: 0.0839 - val_loss: 0.0843 - val_rpn_loss: 0.0837
Epoch 16/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0831 - rpn_loss: 0.0831 - val_loss: 0.0863 - val_rpn_loss: 0.0817
Epoch 17/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0837 - rpn_loss: 0.0837 - val_loss: 0.0804 - val_rpn_loss: 0.0837
Epoch 18/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0822 - rpn_loss: 0.0822 - val_loss: 0.0778 - val_rpn_loss: 0.0814
Epoch 19/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0803 - rpn_loss: 0.0803 - val_loss: 0.0781 - val_rpn_loss: 0.0787
Epoch 20/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0794 - rpn_loss: 0.0794 - val_loss: 0.0736 - val_rpn_loss: 0.0771
Epoch 21/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0776 - rpn_loss: 0.0776 - val_loss: 0.0785 - val_rpn_loss: 0.0763
Epoch 22/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0735 - rpn_loss: 0.0735 - val_loss: 0.0751 - val_rpn_loss: 0.0749
Epoch 23/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0701 - rpn_loss: 0.0701 - val_loss: 0.0774 - val_rpn_loss: 0.0725
Epoch 24/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0696 - rpn_loss: 0.0696 - val_loss: 0.0722 - val_rpn_loss: 0.0680
Epoch 25/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0693 - rpn_loss: 0.0693 - val_loss: 0.0623 - val_rpn_loss: 0.0690
Epoch 26/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0688 - rpn_loss: 0.0688 - val_loss: 0.0762 - val_rpn_loss: 0.0710
Epoch 27/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0702 - rpn_loss: 0.0702 - val_loss: 0.0719 - val_rpn_loss: 0.0737
Epoch 28/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0710 - rpn_loss: 0.0709 - val_loss: 0.0742 - val_rpn_loss: 0.0733
Epoch 29/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0701 - rpn_loss: 0.0700 - val_loss: 0.0594 - val_rpn_loss: 0.0687
Epoch 30/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0676 - rpn_loss: 0.0676 - val_loss: 0.0708 - val_rpn_loss: 0.0684
Epoch 31/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0645 - rpn_loss: 0.0645 - val_loss: 0.0656 - val_rpn_loss: 0.0660
Epoch 32/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0623 - rpn_loss: 0.0623 - val_loss: 0.0607 - val_rpn_loss: 0.0641
Epoch 33/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0630 - rpn_loss: 0.0629 - val_loss: 0.0622 - val_rpn_loss: 0.0645
Epoch 34/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0632 - rpn_loss: 0.0632 - val_loss: 0.0547 - val_rpn_loss: 0.0657
Epoch 35/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0625 - rpn_loss: 0.0625 - val_loss: 0.0778 - val_rpn_loss: 0.0673
Epoch 36/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0641 - rpn_loss: 0.0641 - val_loss: 0.0703 - val_rpn_loss: 0.0683
Epoch 37/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0633 - rpn_loss: 0.0633 - val_loss: 0.0621 - val_rpn_loss: 0.0659
Epoch 38/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0611 - rpn_loss: 0.0611 - val_loss: 0.0620 - val_rpn_loss: 0.0641
Epoch 39/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0589 - rpn_loss: 0.0589 - val_loss: 0.0578 - val_rpn_loss: 0.0626
Epoch 40/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0586 - rpn_loss: 0.0586 - val_loss: 0.0688 - val_rpn_loss: 0.0602
Epoch 41/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0585 - rpn_loss: 0.0585 - val_loss: 0.0710 - val_rpn_loss: 0.0631
Epoch 42/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0570 - rpn_loss: 0.0570 - val_loss: 0.0607 - val_rpn_loss: 0.0631
Epoch 43/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0588 - rpn_loss: 0.0587 - val_loss: 0.0662 - val_rpn_loss: 0.0657
Epoch 44/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0604 - rpn_loss: 0.0603 - val_loss: 0.0643 - val_rpn_loss: 0.0630
Epoch 45/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0594 - rpn_loss: 0.0594 - val_loss: 0.0632 - val_rpn_loss: 0.0627
Epoch 46/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0564 - rpn_loss: 0.0564 - val_loss: 0.0592 - val_rpn_loss: 0.0606
Epoch 47/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0542 - rpn_loss: 0.0542 - val_loss: 0.0640 - val_rpn_loss: 0.0624
Epoch 48/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0545 - rpn_loss: 0.0545 - val_loss: 0.0583 - val_rpn_loss: 0.0585
Epoch 49/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0547 - rpn_loss: 0.0547 - val_loss: 0.0644 - val_rpn_loss: 0.0590
Epoch 50/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0531 - rpn_loss: 0.0531 - val_loss: 0.0644 - val_rpn_loss: 0.0618
Epoch 51/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0550 - rpn_loss: 0.0550 - val_loss: 0.0582 - val_rpn_loss: 0.0632
Epoch 52/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0552 - rpn_loss: 0.0552 - val_loss: 0.0539 - val_rpn_loss: 0.0635
Epoch 53/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0565 - rpn_loss: 0.0565 - val_loss: 0.0598 - val_rpn_loss: 0.0605
Epoch 54/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0536 - rpn_loss: 0.0536 - val_loss: 0.0528 - val_rpn_loss: 0.0607
Epoch 55/500
309/309 [==============================] - 141s 455ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0513 - val_rpn_loss: 0.0588
Epoch 56/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0508 - rpn_loss: 0.0507 - val_loss: 0.0599 - val_rpn_loss: 0.0595
Epoch 57/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0514 - rpn_loss: 0.0514 - val_loss: 0.0596 - val_rpn_loss: 0.0585
Epoch 58/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0614 - val_rpn_loss: 0.0607
Epoch 59/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0536 - val_rpn_loss: 0.0613
Epoch 60/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0540 - rpn_loss: 0.0540 - val_loss: 0.0622 - val_rpn_loss: 0.0614
Epoch 61/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0530 - rpn_loss: 0.0530 - val_loss: 0.0649 - val_rpn_loss: 0.0621
Epoch 62/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0614 - val_rpn_loss: 0.0612
Epoch 63/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0584 - val_rpn_loss: 0.0608
Epoch 64/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0472 - rpn_loss: 0.0472 - val_loss: 0.0550 - val_rpn_loss: 0.0560
##########################
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:624: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:626: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-21 02:51:20.858108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-07-21 02:51:20.858397: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c87480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-21 02:51:20.858428: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-21 02:51:20.863405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-21 02:51:20.970875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:20.971537: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c879c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-21 02:51:20.971573: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-21 02:51:20.971787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:20.972284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-21 02:51:20.972603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:51:20.989566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-21 02:51:20.998311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-21 02:51:21.006272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-21 02:51:21.023043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-21 02:51:21.029024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-21 02:51:21.047928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-21 02:51:21.048089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:21.048677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:21.049176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-21 02:51:21.049246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:51:21.050435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-21 02:51:21.050461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-21 02:51:21.050477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-21 02:51:21.050612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:21.051166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:21.051730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1423: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-21 02:51:40.108164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.108760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-21 02:51:40.108867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:51:40.108886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-21 02:51:40.108897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-21 02:51:40.108910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-21 02:51:40.108930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-21 02:51:40.108941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-21 02:51:40.108954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-21 02:51:40.109029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.109552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.110030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-21 02:51:40.110848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.111373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-21 02:51:40.111421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:51:40.111446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-21 02:51:40.111463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-21 02:51:40.111482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-21 02:51:40.111509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-21 02:51:40.111525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-21 02:51:40.111542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-21 02:51:40.111605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.112140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.112641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-21 02:51:40.112684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-21 02:51:40.112699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-21 02:51:40.112707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-21 02:51:40.112845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.113409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:51:40.113931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-21 02:51:40.302264: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-21 02:51:40.302418: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.0002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200721T0251/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-21 02:52:43.472633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-21 02:52:47.713244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
309/309 [==============================] - 146s 471ms/step - loss: 0.0544 - rpn_loss: 0.0543 - val_loss: 0.0623 - val_rpn_loss: 0.0577
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 130s 420ms/step - loss: 0.0537 - rpn_loss: 0.0537 - val_loss: 0.0579 - val_rpn_loss: 0.0561
Epoch 3/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0538 - rpn_loss: 0.0538 - val_loss: 0.0628 - val_rpn_loss: 0.0576
Epoch 4/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0524 - rpn_loss: 0.0524 - val_loss: 0.0603 - val_rpn_loss: 0.0588
Epoch 5/500
309/309 [==============================] - 89s 286ms/step - loss: 0.0531 - rpn_loss: 0.0531 - val_loss: 0.0570 - val_rpn_loss: 0.0586
Epoch 6/500
309/309 [==============================] - 128s 414ms/step - loss: 0.0533 - rpn_loss: 0.0532 - val_loss: 0.0523 - val_rpn_loss: 0.0580
Epoch 7/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0529 - rpn_loss: 0.0529 - val_loss: 0.0640 - val_rpn_loss: 0.0572
Epoch 8/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0532 - rpn_loss: 0.0532 - val_loss: 0.0647 - val_rpn_loss: 0.0582
Epoch 9/500
309/309 [==============================] - 130s 422ms/step - loss: 0.0533 - rpn_loss: 0.0533 - val_loss: 0.0636 - val_rpn_loss: 0.0582
Epoch 10/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0531 - rpn_loss: 0.0530 - val_loss: 0.0536 - val_rpn_loss: 0.0566
Epoch 11/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0532 - rpn_loss: 0.0532 - val_loss: 0.0678 - val_rpn_loss: 0.0565
Epoch 12/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0522 - rpn_loss: 0.0522 - val_loss: 0.0515 - val_rpn_loss: 0.0572
Epoch 13/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0528 - rpn_loss: 0.0528 - val_loss: 0.0582 - val_rpn_loss: 0.0595
Epoch 14/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0519 - rpn_loss: 0.0519 - val_loss: 0.0597 - val_rpn_loss: 0.0569
Epoch 15/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0526 - rpn_loss: 0.0526 - val_loss: 0.0557 - val_rpn_loss: 0.0580
Epoch 16/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0529 - rpn_loss: 0.0529 - val_loss: 0.0656 - val_rpn_loss: 0.0574
Epoch 17/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0534 - rpn_loss: 0.0534 - val_loss: 0.0626 - val_rpn_loss: 0.0582
Epoch 18/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0612 - val_rpn_loss: 0.0563
Epoch 19/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0597 - val_rpn_loss: 0.0593
Epoch 20/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0524 - rpn_loss: 0.0524 - val_loss: 0.0613 - val_rpn_loss: 0.0571
Epoch 21/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0516 - rpn_loss: 0.0516 - val_loss: 0.0570 - val_rpn_loss: 0.0578
Epoch 22/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0529 - rpn_loss: 0.0529 - val_loss: 0.0715 - val_rpn_loss: 0.0565
Epoch 23/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0530 - rpn_loss: 0.0530 - val_loss: 0.0472 - val_rpn_loss: 0.0580
Epoch 24/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0522 - rpn_loss: 0.0522 - val_loss: 0.0556 - val_rpn_loss: 0.0557
Epoch 25/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0526 - rpn_loss: 0.0526 - val_loss: 0.0508 - val_rpn_loss: 0.0578
Epoch 26/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0646 - val_rpn_loss: 0.0558
Epoch 27/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0516 - rpn_loss: 0.0516 - val_loss: 0.0686 - val_rpn_loss: 0.0573
Epoch 28/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0524 - rpn_loss: 0.0524 - val_loss: 0.0665 - val_rpn_loss: 0.0570
Epoch 29/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0515 - rpn_loss: 0.0515 - val_loss: 0.0584 - val_rpn_loss: 0.0567
Epoch 30/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0517 - rpn_loss: 0.0517 - val_loss: 0.0709 - val_rpn_loss: 0.0568
Epoch 31/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0600 - val_rpn_loss: 0.0568
Epoch 32/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0656 - val_rpn_loss: 0.0585
Epoch 33/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0531 - val_rpn_loss: 0.0569
Epoch 34/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0513 - rpn_loss: 0.0513 - val_loss: 0.0615 - val_rpn_loss: 0.0575
Epoch 35/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0530 - rpn_loss: 0.0530 - val_loss: 0.0571 - val_rpn_loss: 0.0581
Epoch 36/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0517 - rpn_loss: 0.0517 - val_loss: 0.0505 - val_rpn_loss: 0.0568
Epoch 37/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0516 - rpn_loss: 0.0516 - val_loss: 0.0406 - val_rpn_loss: 0.0581
Epoch 38/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0513 - rpn_loss: 0.0513 - val_loss: 0.0633 - val_rpn_loss: 0.0552
Epoch 39/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0521 - rpn_loss: 0.0521 - val_loss: 0.0486 - val_rpn_loss: 0.0587
Epoch 40/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0513 - rpn_loss: 0.0513 - val_loss: 0.0519 - val_rpn_loss: 0.0545
Epoch 41/500
309/309 [==============================] - 138s 445ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0466 - val_rpn_loss: 0.0575
Epoch 42/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0521 - rpn_loss: 0.0521 - val_loss: 0.0519 - val_rpn_loss: 0.0578
Epoch 43/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0512 - rpn_loss: 0.0512 - val_loss: 0.0576 - val_rpn_loss: 0.0535
Epoch 44/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0514 - rpn_loss: 0.0514 - val_loss: 0.0573 - val_rpn_loss: 0.0588
Epoch 45/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0522 - rpn_loss: 0.0522 - val_loss: 0.0568 - val_rpn_loss: 0.0591
Epoch 46/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0527 - val_rpn_loss: 0.0571
Epoch 47/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0524 - rpn_loss: 0.0524 - val_loss: 0.0564 - val_rpn_loss: 0.0568
Epoch 48/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0535 - val_rpn_loss: 0.0575
Epoch 49/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0515 - rpn_loss: 0.0515 - val_loss: 0.0729 - val_rpn_loss: 0.0580
Epoch 50/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0518 - rpn_loss: 0.0518 - val_loss: 0.0589 - val_rpn_loss: 0.0583
Epoch 51/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0583 - val_rpn_loss: 0.0569
Epoch 52/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0513 - rpn_loss: 0.0513 - val_loss: 0.0576 - val_rpn_loss: 0.0582
Epoch 53/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0557 - val_rpn_loss: 0.0573
Epoch 54/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0523 - rpn_loss: 0.0522 - val_loss: 0.0570 - val_rpn_loss: 0.0552
Epoch 55/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0601 - val_rpn_loss: 0.0579
Epoch 56/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0509 - rpn_loss: 0.0509 - val_loss: 0.0675 - val_rpn_loss: 0.0573
Epoch 57/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0509 - rpn_loss: 0.0509 - val_loss: 0.0639 - val_rpn_loss: 0.0574
Epoch 58/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0658 - val_rpn_loss: 0.0562
Epoch 59/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0515 - rpn_loss: 0.0515 - val_loss: 0.0557 - val_rpn_loss: 0.0561
Epoch 60/500
309/309 [==============================] - 138s 445ms/step - loss: 0.0515 - rpn_loss: 0.0515 - val_loss: 0.0656 - val_rpn_loss: 0.0582
Epoch 61/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0703 - val_rpn_loss: 0.0564
Epoch 62/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0497 - val_rpn_loss: 0.0579
Epoch 63/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0507 - rpn_loss: 0.0507 - val_loss: 0.0541 - val_rpn_loss: 0.0556
Epoch 64/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0512 - rpn_loss: 0.0512 - val_loss: 0.0653 - val_rpn_loss: 0.0573
Epoch 65/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0518 - rpn_loss: 0.0518 - val_loss: 0.0498 - val_rpn_loss: 0.0567
Epoch 66/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0508 - rpn_loss: 0.0507 - val_loss: 0.0598 - val_rpn_loss: 0.0572
Epoch 67/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0508 - rpn_loss: 0.0507 - val_loss: 0.0535 - val_rpn_loss: 0.0568
Epoch 68/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0630 - val_rpn_loss: 0.0599
Epoch 69/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0578 - val_rpn_loss: 0.0566
Epoch 70/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0667 - val_rpn_loss: 0.0582
Epoch 71/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0519 - rpn_loss: 0.0519 - val_loss: 0.0584 - val_rpn_loss: 0.0584
Epoch 72/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0389 - val_rpn_loss: 0.0540
Epoch 73/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0679 - val_rpn_loss: 0.0555
Epoch 74/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0518 - rpn_loss: 0.0518 - val_loss: 0.0486 - val_rpn_loss: 0.0574
Epoch 75/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0681 - val_rpn_loss: 0.0575
Epoch 76/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0554 - val_rpn_loss: 0.0581
Epoch 77/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0503 - rpn_loss: 0.0503 - val_loss: 0.0637 - val_rpn_loss: 0.0590
Epoch 78/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0511 - val_rpn_loss: 0.0546
Epoch 79/500
309/309 [==============================] - 136s 442ms/step - loss: 0.0512 - rpn_loss: 0.0512 - val_loss: 0.0553 - val_rpn_loss: 0.0571
Epoch 80/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0659 - val_rpn_loss: 0.0563
Epoch 81/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0506 - val_rpn_loss: 0.0581
Epoch 82/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0585 - val_rpn_loss: 0.0574
Epoch 83/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0586 - val_rpn_loss: 0.0570
Epoch 84/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0675 - val_rpn_loss: 0.0577
Epoch 85/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0507 - rpn_loss: 0.0507 - val_loss: 0.0570 - val_rpn_loss: 0.0552
Epoch 86/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0479 - val_rpn_loss: 0.0572
Epoch 87/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0536 - val_rpn_loss: 0.0578
Epoch 88/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0507 - rpn_loss: 0.0507 - val_loss: 0.0633 - val_rpn_loss: 0.0576
Epoch 89/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0514 - val_rpn_loss: 0.0567
Epoch 90/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0508 - rpn_loss: 0.0507 - val_loss: 0.0544 - val_rpn_loss: 0.0580
Epoch 91/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0503 - rpn_loss: 0.0503 - val_loss: 0.0540 - val_rpn_loss: 0.0581
Epoch 92/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0616 - val_rpn_loss: 0.0577
Epoch 93/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0589 - val_rpn_loss: 0.0576
Epoch 94/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0504 - rpn_loss: 0.0504 - val_loss: 0.0584 - val_rpn_loss: 0.0548
Epoch 95/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0555 - val_rpn_loss: 0.0546
Epoch 96/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0595 - val_rpn_loss: 0.0557
Epoch 97/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0540 - val_rpn_loss: 0.0599
Epoch 98/500
309/309 [==============================] - 132s 426ms/step - loss: 0.0516 - rpn_loss: 0.0516 - val_loss: 0.0465 - val_rpn_loss: 0.0573
Epoch 99/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0529 - val_rpn_loss: 0.0571
Epoch 100/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0530 - val_rpn_loss: 0.0574
Epoch 101/500
309/309 [==============================] - 131s 425ms/step - loss: 0.0506 - rpn_loss: 0.0506 - val_loss: 0.0577 - val_rpn_loss: 0.0566
Epoch 102/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0500 - rpn_loss: 0.0499 - val_loss: 0.0561 - val_rpn_loss: 0.0560
Epoch 103/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0502 - rpn_loss: 0.0502 - val_loss: 0.0697 - val_rpn_loss: 0.0577
Epoch 104/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0504 - rpn_loss: 0.0504 - val_loss: 0.0520 - val_rpn_loss: 0.0545
Epoch 105/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0505 - val_rpn_loss: 0.0583
Epoch 106/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0507 - rpn_loss: 0.0507 - val_loss: 0.0535 - val_rpn_loss: 0.0573
Epoch 107/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0572 - val_rpn_loss: 0.0588
Epoch 108/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0629 - val_rpn_loss: 0.0566
Epoch 109/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0504 - rpn_loss: 0.0504 - val_loss: 0.0572 - val_rpn_loss: 0.0577
Epoch 110/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0507 - rpn_loss: 0.0507 - val_loss: 0.0626 - val_rpn_loss: 0.0569
Epoch 111/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0497 - rpn_loss: 0.0496 - val_loss: 0.0571 - val_rpn_loss: 0.0555
Epoch 112/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0645 - val_rpn_loss: 0.0574
Epoch 113/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0580 - val_rpn_loss: 0.0583
Epoch 114/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0510 - rpn_loss: 0.0510 - val_loss: 0.0485 - val_rpn_loss: 0.0572
Epoch 115/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0503 - rpn_loss: 0.0503 - val_loss: 0.0536 - val_rpn_loss: 0.0578
Epoch 116/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0482 - val_rpn_loss: 0.0563
Epoch 117/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0587 - val_rpn_loss: 0.0561
Epoch 118/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0502 - rpn_loss: 0.0502 - val_loss: 0.0619 - val_rpn_loss: 0.0557
Epoch 119/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0511 - rpn_loss: 0.0511 - val_loss: 0.0611 - val_rpn_loss: 0.0581
Epoch 120/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0625 - val_rpn_loss: 0.0578
Epoch 121/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0529 - val_rpn_loss: 0.0562
Epoch 122/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0614 - val_rpn_loss: 0.0584
Epoch 123/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0512 - rpn_loss: 0.0512 - val_loss: 0.0617 - val_rpn_loss: 0.0574
Epoch 124/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0627 - val_rpn_loss: 0.0566
Epoch 125/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0541 - val_rpn_loss: 0.0583
Epoch 126/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0505 - val_rpn_loss: 0.0552
Epoch 127/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0595 - val_rpn_loss: 0.0579
Epoch 128/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0504 - rpn_loss: 0.0504 - val_loss: 0.0513 - val_rpn_loss: 0.0562
Epoch 129/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0491 - rpn_loss: 0.0491 - val_loss: 0.0630 - val_rpn_loss: 0.0582
Epoch 130/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0570 - val_rpn_loss: 0.0562
Epoch 131/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0648 - val_rpn_loss: 0.0586
Epoch 132/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0533 - val_rpn_loss: 0.0569
Epoch 133/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0500 - rpn_loss: 0.0500 - val_loss: 0.0658 - val_rpn_loss: 0.0574
Epoch 134/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0555 - val_rpn_loss: 0.0556
Epoch 135/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0499 - val_rpn_loss: 0.0576
Epoch 136/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0500 - rpn_loss: 0.0499 - val_loss: 0.0535 - val_rpn_loss: 0.0550
Epoch 137/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0620 - val_rpn_loss: 0.0570
Epoch 138/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0688 - val_rpn_loss: 0.0584
Epoch 139/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0505 - rpn_loss: 0.0505 - val_loss: 0.0622 - val_rpn_loss: 0.0579
Epoch 140/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0501 - rpn_loss: 0.0501 - val_loss: 0.0647 - val_rpn_loss: 0.0578
Epoch 141/500
309/309 [==============================] - 135s 435ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0517 - val_rpn_loss: 0.0582
Epoch 142/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0499 - rpn_loss: 0.0499 - val_loss: 0.0662 - val_rpn_loss: 0.0547
Epoch 143/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0489 - rpn_loss: 0.0489 - val_loss: 0.0627 - val_rpn_loss: 0.0583
Epoch 144/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0606 - val_rpn_loss: 0.0572
Epoch 145/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0489 - val_rpn_loss: 0.0572
Epoch 146/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0569 - val_rpn_loss: 0.0579
Epoch 147/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0491 - rpn_loss: 0.0491 - val_loss: 0.0637 - val_rpn_loss: 0.0567
Epoch 148/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0693 - val_rpn_loss: 0.0557
Epoch 149/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0609 - val_rpn_loss: 0.0583
Epoch 150/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0494 - rpn_loss: 0.0494 - val_loss: 0.0654 - val_rpn_loss: 0.0575
Epoch 151/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0640 - val_rpn_loss: 0.0557
Epoch 152/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0489 - val_rpn_loss: 0.0562
Epoch 153/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0577 - val_rpn_loss: 0.0575
Epoch 154/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0494 - rpn_loss: 0.0493 - val_loss: 0.0485 - val_rpn_loss: 0.0591
Epoch 155/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0563 - val_rpn_loss: 0.0567
Epoch 156/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0597 - val_rpn_loss: 0.0574
Epoch 157/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0545 - val_rpn_loss: 0.0565
Epoch 158/500
309/309 [==============================] - 135s 435ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0432 - val_rpn_loss: 0.0561
Epoch 159/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0664 - val_rpn_loss: 0.0580
Epoch 160/500
309/309 [==============================] - 134s 432ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0607 - val_rpn_loss: 0.0585
Epoch 161/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0558 - val_rpn_loss: 0.0573
Epoch 162/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0447 - val_rpn_loss: 0.0557
Epoch 163/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0495 - rpn_loss: 0.0494 - val_loss: 0.0491 - val_rpn_loss: 0.0555
Epoch 164/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0614 - val_rpn_loss: 0.0559
Epoch 165/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0535 - val_rpn_loss: 0.0588
Epoch 166/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0504 - val_rpn_loss: 0.0569
Epoch 167/500
309/309 [==============================] - 132s 428ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0643 - val_rpn_loss: 0.0561
Epoch 168/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0494 - rpn_loss: 0.0494 - val_loss: 0.0679 - val_rpn_loss: 0.0561
Epoch 169/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0488 - rpn_loss: 0.0488 - val_loss: 0.0775 - val_rpn_loss: 0.0578
Epoch 170/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0620 - val_rpn_loss: 0.0578
Epoch 171/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0499 - rpn_loss: 0.0499 - val_loss: 0.0652 - val_rpn_loss: 0.0584
Epoch 172/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0488 - rpn_loss: 0.0488 - val_loss: 0.0537 - val_rpn_loss: 0.0567
Epoch 173/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0497 - rpn_loss: 0.0497 - val_loss: 0.0605 - val_rpn_loss: 0.0573
Epoch 174/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0521 - val_rpn_loss: 0.0580
Epoch 175/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0480 - rpn_loss: 0.0480 - val_loss: 0.0627 - val_rpn_loss: 0.0557
Epoch 176/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0616 - val_rpn_loss: 0.0567
Epoch 177/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0489 - rpn_loss: 0.0489 - val_loss: 0.0683 - val_rpn_loss: 0.0572
Epoch 178/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0490 - rpn_loss: 0.0489 - val_loss: 0.0669 - val_rpn_loss: 0.0587
Epoch 179/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0494 - rpn_loss: 0.0494 - val_loss: 0.0559 - val_rpn_loss: 0.0563
Epoch 180/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0597 - val_rpn_loss: 0.0567
Epoch 181/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0511 - val_rpn_loss: 0.0578
Epoch 182/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0527 - val_rpn_loss: 0.0569
Epoch 183/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0452 - val_rpn_loss: 0.0566
Epoch 184/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0442 - val_rpn_loss: 0.0552
Epoch 185/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0490 - rpn_loss: 0.0489 - val_loss: 0.0556 - val_rpn_loss: 0.0588
Epoch 186/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0589 - val_rpn_loss: 0.0590
Epoch 187/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0494 - rpn_loss: 0.0493 - val_loss: 0.0609 - val_rpn_loss: 0.0567
Epoch 188/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0489 - rpn_loss: 0.0489 - val_loss: 0.0597 - val_rpn_loss: 0.0552
Epoch 189/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0534 - val_rpn_loss: 0.0584
Epoch 190/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0477 - val_rpn_loss: 0.0585
Epoch 191/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0496 - rpn_loss: 0.0496 - val_loss: 0.0544 - val_rpn_loss: 0.0572
Epoch 192/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0482 - rpn_loss: 0.0482 - val_loss: 0.0536 - val_rpn_loss: 0.0569
Epoch 193/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0580 - val_rpn_loss: 0.0558
Epoch 194/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0486 - rpn_loss: 0.0486 - val_loss: 0.0443 - val_rpn_loss: 0.0568
Epoch 195/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0491 - rpn_loss: 0.0491 - val_loss: 0.0636 - val_rpn_loss: 0.0562
Epoch 196/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0610 - val_rpn_loss: 0.0588
Epoch 197/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0604 - val_rpn_loss: 0.0577
Epoch 198/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0638 - val_rpn_loss: 0.0544
Epoch 199/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0576 - val_rpn_loss: 0.0561
Epoch 200/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0488 - rpn_loss: 0.0488 - val_loss: 0.0656 - val_rpn_loss: 0.0566
Epoch 201/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0593 - val_rpn_loss: 0.0596
Epoch 202/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0481 - rpn_loss: 0.0480 - val_loss: 0.0505 - val_rpn_loss: 0.0578
Epoch 203/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0491 - rpn_loss: 0.0491 - val_loss: 0.0523 - val_rpn_loss: 0.0568
Epoch 204/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0472 - val_rpn_loss: 0.0562
Epoch 205/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0486 - rpn_loss: 0.0486 - val_loss: 0.0570 - val_rpn_loss: 0.0576
Epoch 206/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0482 - rpn_loss: 0.0482 - val_loss: 0.0623 - val_rpn_loss: 0.0549
Epoch 207/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0483 - rpn_loss: 0.0483 - val_loss: 0.0623 - val_rpn_loss: 0.0577
Epoch 208/500
309/309 [==============================] - 137s 442ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0548 - val_rpn_loss: 0.0575
Epoch 209/500
309/309 [==============================] - 135s 437ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0580 - val_rpn_loss: 0.0574
Epoch 210/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0542 - val_rpn_loss: 0.0550
Epoch 211/500
309/309 [==============================] - 136s 440ms/step - loss: 0.0484 - rpn_loss: 0.0484 - val_loss: 0.0650 - val_rpn_loss: 0.0582
Epoch 212/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0484 - rpn_loss: 0.0484 - val_loss: 0.0607 - val_rpn_loss: 0.0585
Epoch 213/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0488 - rpn_loss: 0.0488 - val_loss: 0.0667 - val_rpn_loss: 0.0566
Epoch 214/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0693 - val_rpn_loss: 0.0582
Epoch 215/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0579 - val_rpn_loss: 0.0580
Epoch 216/500
309/309 [==============================] - 133s 430ms/step - loss: 0.0491 - rpn_loss: 0.0491 - val_loss: 0.0557 - val_rpn_loss: 0.0565
Epoch 217/500
309/309 [==============================] - 132s 427ms/step - loss: 0.0490 - rpn_loss: 0.0490 - val_loss: 0.0491 - val_rpn_loss: 0.0577
Epoch 218/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0660 - val_rpn_loss: 0.0575
Epoch 219/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0483 - rpn_loss: 0.0483 - val_loss: 0.0612 - val_rpn_loss: 0.0576
Epoch 220/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0484 - rpn_loss: 0.0484 - val_loss: 0.0383 - val_rpn_loss: 0.0559
Epoch 221/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0484 - rpn_loss: 0.0484 - val_loss: 0.0658 - val_rpn_loss: 0.0559
Epoch 222/500
309/309 [==============================] - 133s 429ms/step - loss: 0.0477 - rpn_loss: 0.0476 - val_loss: 0.0663 - val_rpn_loss: 0.0578
Epoch 223/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0484 - rpn_loss: 0.0484 - val_loss: 0.0561 - val_rpn_loss: 0.0577
Epoch 224/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0493 - rpn_loss: 0.0493 - val_loss: 0.0675 - val_rpn_loss: 0.0580
Epoch 225/500
 59/309 [====>.........................] - ETA: 2:18 - loss: 0.0482 - rpn_loss: 0.0482
