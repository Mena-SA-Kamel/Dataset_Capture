WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:639: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:641: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-24 15:45:13.848867: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-07-24 15:45:13.849133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x228d480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-24 15:45:13.849163: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-24 15:45:13.850987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-24 15:45:13.959188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:13.959853: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x228d9c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-24 15:45:13.959883: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-24 15:45:13.960038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:13.960605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-24 15:45:13.960919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-24 15:45:13.962631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-24 15:45:13.964240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-24 15:45:13.964620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-24 15:45:13.966337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-24 15:45:13.967055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-24 15:45:13.970424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-24 15:45:13.970533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:13.971062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:13.971570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-24 15:45:13.971645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-24 15:45:13.972797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-24 15:45:13.972819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-24 15:45:13.972826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-24 15:45:13.972929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:13.973529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:13.974069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1423: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-24 15:45:19.194692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.195313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-24 15:45:19.195403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-24 15:45:19.195424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-24 15:45:19.195441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-24 15:45:19.195457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-24 15:45:19.195471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-24 15:45:19.195486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-24 15:45:19.195502: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-24 15:45:19.195571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.196083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.196572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-24 15:45:19.197276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.197785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-24 15:45:19.197832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-24 15:45:19.197854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-24 15:45:19.197868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-24 15:45:19.197881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-24 15:45:19.197893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-24 15:45:19.197908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-24 15:45:19.197923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-24 15:45:19.197970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.198494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.198963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-24 15:45:19.199002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-24 15:45:19.199012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-24 15:45:19.199018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-24 15:45:19.199098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.199663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-24 15:45:19.200184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-24 15:45:19.392781: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-24 15:45:19.392985: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200724T1545/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-24 15:46:25.346529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-24 15:46:29.565314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
  2/309 [..............................] - ETA: 2:00:19 - loss: 0.1721 - rpn_loss: 0.1721/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.468383). Check your callbacks.
  % (hook_name, delta_t_median), RuntimeWarning)
309/309 [==============================] - 154s 497ms/step - loss: 0.1009 - rpn_loss: 0.1009 - val_loss: 0.0964 - val_rpn_loss: 0.0951
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0952 - rpn_loss: 0.0952 - val_loss: 0.0896 - val_rpn_loss: 0.0904
Epoch 3/500
309/309 [==============================] - 140s 455ms/step - loss: 0.0949 - rpn_loss: 0.0948 - val_loss: 0.0916 - val_rpn_loss: 0.0924
Epoch 4/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0941 - rpn_loss: 0.0941 - val_loss: 0.0913 - val_rpn_loss: 0.0910
Epoch 5/500
309/309 [==============================] - 102s 332ms/step - loss: 0.0923 - rpn_loss: 0.0923 - val_loss: 0.0890 - val_rpn_loss: 0.0904
Epoch 6/500
309/309 [==============================] - 129s 419ms/step - loss: 0.0915 - rpn_loss: 0.0915 - val_loss: 0.0881 - val_rpn_loss: 0.0892
Epoch 7/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0916 - rpn_loss: 0.0916 - val_loss: 0.0919 - val_rpn_loss: 0.0907
Epoch 8/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0904 - rpn_loss: 0.0904 - val_loss: 0.0906 - val_rpn_loss: 0.0891
Epoch 9/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0900 - rpn_loss: 0.0900 - val_loss: 0.0873 - val_rpn_loss: 0.0869
Epoch 10/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0897 - rpn_loss: 0.0897 - val_loss: 0.0840 - val_rpn_loss: 0.0874
Epoch 11/500
309/309 [==============================] - 141s 458ms/step - loss: 0.0885 - rpn_loss: 0.0884 - val_loss: 0.0839 - val_rpn_loss: 0.0878
Epoch 12/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0873 - rpn_loss: 0.0873 - val_loss: 0.0850 - val_rpn_loss: 0.0857
Epoch 13/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0845 - rpn_loss: 0.0845 - val_loss: 0.0781 - val_rpn_loss: 0.0820
Epoch 14/500
309/309 [==============================] - 141s 458ms/step - loss: 0.0808 - rpn_loss: 0.0807 - val_loss: 0.0798 - val_rpn_loss: 0.0808
Epoch 15/500
309/309 [==============================] - 142s 459ms/step - loss: 0.0798 - rpn_loss: 0.0798 - val_loss: 0.0808 - val_rpn_loss: 0.0780
Epoch 16/500
309/309 [==============================] - 142s 459ms/step - loss: 0.0774 - rpn_loss: 0.0774 - val_loss: 0.0725 - val_rpn_loss: 0.0750
Epoch 17/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0755 - rpn_loss: 0.0755 - val_loss: 0.0780 - val_rpn_loss: 0.0754
Epoch 18/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0759 - rpn_loss: 0.0759 - val_loss: 0.0802 - val_rpn_loss: 0.0775
Epoch 19/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0773 - rpn_loss: 0.0772 - val_loss: 0.0761 - val_rpn_loss: 0.0790
Epoch 20/500
309/309 [==============================] - 141s 458ms/step - loss: 0.0772 - rpn_loss: 0.0772 - val_loss: 0.0767 - val_rpn_loss: 0.0760
Epoch 21/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0736 - rpn_loss: 0.0736 - val_loss: 0.0693 - val_rpn_loss: 0.0725
Epoch 22/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0719 - rpn_loss: 0.0719 - val_loss: 0.0769 - val_rpn_loss: 0.0708
Epoch 23/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0690 - rpn_loss: 0.0690 - val_loss: 0.0691 - val_rpn_loss: 0.0718
Epoch 24/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0678 - rpn_loss: 0.0678 - val_loss: 0.0694 - val_rpn_loss: 0.0684
Epoch 25/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0670 - rpn_loss: 0.0670 - val_loss: 0.0613 - val_rpn_loss: 0.0676
Epoch 26/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0661 - rpn_loss: 0.0661 - val_loss: 0.0734 - val_rpn_loss: 0.0700
Epoch 27/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0666 - rpn_loss: 0.0666 - val_loss: 0.0759 - val_rpn_loss: 0.0687
Epoch 28/500
309/309 [==============================] - 142s 458ms/step - loss: 0.0687 - rpn_loss: 0.0687 - val_loss: 0.0638 - val_rpn_loss: 0.0723
Epoch 29/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0660 - rpn_loss: 0.0660 - val_loss: 0.0661 - val_rpn_loss: 0.0664
Epoch 30/500
309/309 [==============================] - 142s 459ms/step - loss: 0.0626 - rpn_loss: 0.0626 - val_loss: 0.0658 - val_rpn_loss: 0.0655
Epoch 31/500
309/309 [==============================] - 142s 458ms/step - loss: 0.0610 - rpn_loss: 0.0610 - val_loss: 0.0608 - val_rpn_loss: 0.0642
Epoch 32/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0596 - rpn_loss: 0.0596 - val_loss: 0.0613 - val_rpn_loss: 0.0630
Epoch 33/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0586 - rpn_loss: 0.0586 - val_loss: 0.0610 - val_rpn_loss: 0.0620
Epoch 34/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0603 - rpn_loss: 0.0603 - val_loss: 0.0542 - val_rpn_loss: 0.0652
Epoch 35/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0600 - rpn_loss: 0.0599 - val_loss: 0.0683 - val_rpn_loss: 0.0645
Epoch 36/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0620 - rpn_loss: 0.0620 - val_loss: 0.0683 - val_rpn_loss: 0.0643
Epoch 37/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0621 - rpn_loss: 0.0621 - val_loss: 0.0670 - val_rpn_loss: 0.0662
Epoch 38/500
309/309 [==============================] - 141s 455ms/step - loss: 0.0591 - rpn_loss: 0.0590 - val_loss: 0.0654 - val_rpn_loss: 0.0626
Epoch 39/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0563 - rpn_loss: 0.0563 - val_loss: 0.0679 - val_rpn_loss: 0.0615
Epoch 40/500
309/309 [==============================] - 143s 461ms/step - loss: 0.0565 - rpn_loss: 0.0565 - val_loss: 0.0621 - val_rpn_loss: 0.0586
Epoch 41/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0552 - rpn_loss: 0.0552 - val_loss: 0.0717 - val_rpn_loss: 0.0629
Epoch 42/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0545 - rpn_loss: 0.0545 - val_loss: 0.0649 - val_rpn_loss: 0.0653
Epoch 43/500
309/309 [==============================] - 142s 461ms/step - loss: 0.0562 - rpn_loss: 0.0562 - val_loss: 0.0597 - val_rpn_loss: 0.0638
Epoch 44/500
309/309 [==============================] - 142s 458ms/step - loss: 0.0573 - rpn_loss: 0.0573 - val_loss: 0.0627 - val_rpn_loss: 0.0612
Epoch 45/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0581 - rpn_loss: 0.0581 - val_loss: 0.0636 - val_rpn_loss: 0.0615
Epoch 46/500
309/309 [==============================] - 142s 460ms/step - loss: 0.0531 - rpn_loss: 0.0531 - val_loss: 0.0712 - val_rpn_loss: 0.0608
Epoch 47/500
309/309 [==============================] - 142s 458ms/step - loss: 0.0530 - rpn_loss: 0.0530 - val_loss: 0.0627 - val_rpn_loss: 0.0581
Epoch 48/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0524 - rpn_loss: 0.0524 - val_loss: 0.0652 - val_rpn_loss: 0.0596
Epoch 49/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0522 - rpn_loss: 0.0522 - val_loss: 0.0529 - val_rpn_loss: 0.0606
Epoch 50/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0520 - rpn_loss: 0.0520 - val_loss: 0.0589 - val_rpn_loss: 0.0608
Epoch 51/500
309/309 [==============================] - 141s 455ms/step - loss: 0.0527 - rpn_loss: 0.0527 - val_loss: 0.0623 - val_rpn_loss: 0.0621
Epoch 52/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0549 - rpn_loss: 0.0549 - val_loss: 0.0606 - val_rpn_loss: 0.0626
Epoch 53/500
309/309 [==============================] - 139s 449ms/step - loss: 0.0552 - rpn_loss: 0.0552 - val_loss: 0.0587 - val_rpn_loss: 0.0584
Epoch 54/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0524 - rpn_loss: 0.0524 - val_loss: 0.0636 - val_rpn_loss: 0.0621
Epoch 55/500
309/309 [==============================] - 141s 457ms/step - loss: 0.0488 - rpn_loss: 0.0488 - val_loss: 0.0689 - val_rpn_loss: 0.0609
Epoch 56/500
309/309 [==============================] - 142s 458ms/step - loss: 0.0483 - rpn_loss: 0.0483 - val_loss: 0.0601 - val_rpn_loss: 0.0585
Epoch 57/500
309/309 [==============================] - 141s 455ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0632 - val_rpn_loss: 0.0598
Epoch 58/500
309/309 [==============================] - 142s 461ms/step - loss: 0.0495 - rpn_loss: 0.0495 - val_loss: 0.0601 - val_rpn_loss: 0.0595
Epoch 59/500
309/309 [==============================] - 141s 458ms/step - loss: 0.0509 - rpn_loss: 0.0509 - val_loss: 0.0604 - val_rpn_loss: 0.0608
Epoch 60/500
309/309 [==============================] - 141s 456ms/step - loss: 0.0511 - rpn_loss: 0.0510 - val_loss: 0.0583 - val_rpn_loss: 0.0647
Epoch 61/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0536 - rpn_loss: 0.0536 - val_loss: 0.0604 - val_rpn_loss: 0.0649
Epoch 62/500
309/309 [==============================] - 142s 458ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0613 - val_rpn_loss: 0.0608
Epoch 63/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0486 - rpn_loss: 0.0486 - val_loss: 0.0635 - val_rpn_loss: 0.0573
Epoch 64/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0454 - rpn_loss: 0.0454 - val_loss: 0.0666 - val_rpn_loss: 0.0573
Epoch 65/500
309/309 [==============================] - 140s 454ms/step - loss: 0.0463 - rpn_loss: 0.0463 - val_loss: 0.0523 - val_rpn_loss: 0.0559
Epoch 66/500
309/309 [==============================] - 142s 461ms/step - loss: 0.0464 - rpn_loss: 0.0464 - val_loss: 0.0527 - val_rpn_loss: 0.0615
Epoch 67/500
309/309 [==============================] - 144s 465ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0717 - val_rpn_loss: 0.0641
Epoch 68/500
309/309 [==============================] - 140s 453ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0555 - val_rpn_loss: 0.0634
Epoch 69/500
309/309 [==============================] - 138s 446ms/step - loss: 0.0498 - rpn_loss: 0.0498 - val_loss: 0.0685 - val_rpn_loss: 0.0643
Epoch 70/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0487 - rpn_loss: 0.0487 - val_loss: 0.0653 - val_rpn_loss: 0.0614
Epoch 71/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0471 - rpn_loss: 0.0470 - val_loss: 0.0676 - val_rpn_loss: 0.0560
Epoch 72/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0452 - rpn_loss: 0.0452 - val_loss: 0.0610 - val_rpn_loss: 0.0579
Epoch 73/500
309/309 [==============================] - 138s 447ms/step - loss: 0.0431 - rpn_loss: 0.0431 - val_loss: 0.0549 - val_rpn_loss: 0.0598
Epoch 74/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0431 - rpn_loss: 0.0431 - val_loss: 0.0644 - val_rpn_loss: 0.0597
Epoch 75/500
309/309 [==============================] - 138s 448ms/step - loss: 0.0454 - rpn_loss: 0.0454 - val_loss: 0.0562 - val_rpn_loss: 0.0619
Epoch 76/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0492 - rpn_loss: 0.0492 - val_loss: 0.0638 - val_rpn_loss: 0.0636
Epoch 77/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0477 - rpn_loss: 0.0476 - val_loss: 0.0710 - val_rpn_loss: 0.0626
Epoch 78/500
309/309 [==============================] - 140s 452ms/step - loss: 0.0450 - rpn_loss: 0.0450 - val_loss: 0.0602 - val_rpn_loss: 0.0611
Epoch 79/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0428 - rpn_loss: 0.0428 - val_loss: 0.0517 - val_rpn_loss: 0.0597
Epoch 80/500
309/309 [==============================] - 139s 450ms/step - loss: 0.0425 - rpn_loss: 0.0425 - val_loss: 0.0537 - val_rpn_loss: 0.0592

########################### train_#13b
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From grasping_points.py:639: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From grasping_points.py:641: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-25 00:30:17.310112: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-07-25 00:30:17.310398: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1427480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-25 00:30:17.310428: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-25 00:30:17.312320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-25 00:30:17.417906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:17.418596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14279c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-25 00:30:17.418635: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-07-25 00:30:17.418797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:17.419331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-25 00:30:17.419613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-25 00:30:17.421066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-25 00:30:17.422589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-25 00:30:17.422911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-25 00:30:17.424302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-25 00:30:17.425013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-25 00:30:17.428036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-25 00:30:17.428156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:17.428698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:17.429256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-25 00:30:17.429328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-25 00:30:17.430542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-25 00:30:17.430563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-25 00:30:17.430570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-25 00:30:17.430669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:17.431227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:17.431735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1423: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-07-25 00:30:22.304497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.305140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-25 00:30:22.305247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-25 00:30:22.305276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-25 00:30:22.305300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-25 00:30:22.305321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-25 00:30:22.305340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-25 00:30:22.305360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-25 00:30:22.305380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-25 00:30:22.305474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.306027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.306534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-25 00:30:22.307245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.307762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-07-25 00:30:22.307819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-25 00:30:22.307848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-25 00:30:22.307870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-25 00:30:22.307893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-25 00:30:22.307915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-25 00:30:22.307935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-25 00:30:22.307954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-25 00:30:22.308020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.308575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.309058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-25 00:30:22.309115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-25 00:30:22.309134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-25 00:30:22.309144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-25 00:30:22.309245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.309773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-25 00:30:22.310280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2020-07-25 00:30:22.505475: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable (VariableV2) /device:GPU:0
  anchors_1/Variable/Assign (Assign) /device:GPU:0
  anchors_1/Variable/read (Identity) /device:GPU:0

2020-07-25 00:30:22.505630: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
VariableV2: CPU
Assign: GPU CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  anchors_1/Variable_1 (VariableV2) /device:GPU:0
  anchors_1/Variable_1/Assign (Assign) /device:GPU:0
  anchors_1/Variable_1/read (Identity) /device:GPU:0


Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/training_logs/grasping_points20200725T0030/mask_rcnn_grasping_points_{epoch:04d}.h5
Selecting layers to train
conv1                  (Conv2D)
bn_conv1               (BatchNorm)
res2a_branch2a         (Conv2D)
bn2a_branch2a          (BatchNorm)
res2a_branch2b         (Conv2D)
bn2a_branch2b          (BatchNorm)
res2a_branch2c         (Conv2D)
res2a_branch1          (Conv2D)
bn2a_branch2c          (BatchNorm)
bn2a_branch1           (BatchNorm)
res2b_branch2a         (Conv2D)
bn2b_branch2a          (BatchNorm)
res2b_branch2b         (Conv2D)
bn2b_branch2b          (BatchNorm)
res2b_branch2c         (Conv2D)
bn2b_branch2c          (BatchNorm)
res2c_branch2a         (Conv2D)
bn2c_branch2a          (BatchNorm)
res2c_branch2b         (Conv2D)
bn2c_branch2b          (BatchNorm)
res2c_branch2c         (Conv2D)
bn2c_branch2c          (BatchNorm)
res3a_branch2a         (Conv2D)
bn3a_branch2a          (BatchNorm)
res3a_branch2b         (Conv2D)
bn3a_branch2b          (BatchNorm)
res3a_branch2c         (Conv2D)
res3a_branch1          (Conv2D)
bn3a_branch2c          (BatchNorm)
bn3a_branch1           (BatchNorm)
res3b_branch2a         (Conv2D)
bn3b_branch2a          (BatchNorm)
res3b_branch2b         (Conv2D)
bn3b_branch2b          (BatchNorm)
res3b_branch2c         (Conv2D)
bn3b_branch2c          (BatchNorm)
res3c_branch2a         (Conv2D)
bn3c_branch2a          (BatchNorm)
res3c_branch2b         (Conv2D)
bn3c_branch2b          (BatchNorm)
res3c_branch2c         (Conv2D)
bn3c_branch2c          (BatchNorm)
res3d_branch2a         (Conv2D)
bn3d_branch2a          (BatchNorm)
res3d_branch2b         (Conv2D)
bn3d_branch2b          (BatchNorm)
res3d_branch2c         (Conv2D)
bn3d_branch2c          (BatchNorm)
res4a_branch2a         (Conv2D)
bn4a_branch2a          (BatchNorm)
res4a_branch2b         (Conv2D)
bn4a_branch2b          (BatchNorm)
res4a_branch2c         (Conv2D)
res4a_branch1          (Conv2D)
bn4a_branch2c          (BatchNorm)
bn4a_branch1           (BatchNorm)
res4b_branch2a         (Conv2D)
bn4b_branch2a          (BatchNorm)
res4b_branch2b         (Conv2D)
bn4b_branch2b          (BatchNorm)
res4b_branch2c         (Conv2D)
bn4b_branch2c          (BatchNorm)
res4c_branch2a         (Conv2D)
bn4c_branch2a          (BatchNorm)
res4c_branch2b         (Conv2D)
bn4c_branch2b          (BatchNorm)
res4c_branch2c         (Conv2D)
bn4c_branch2c          (BatchNorm)
res4d_branch2a         (Conv2D)
bn4d_branch2a          (BatchNorm)
res4d_branch2b         (Conv2D)
bn4d_branch2b          (BatchNorm)
res4d_branch2c         (Conv2D)
bn4d_branch2c          (BatchNorm)
res4e_branch2a         (Conv2D)
bn4e_branch2a          (BatchNorm)
res4e_branch2b         (Conv2D)
bn4e_branch2b          (BatchNorm)
res4e_branch2c         (Conv2D)
bn4e_branch2c          (BatchNorm)
res4f_branch2a         (Conv2D)
bn4f_branch2a          (BatchNorm)
res4f_branch2b         (Conv2D)
bn4f_branch2b          (BatchNorm)
res4f_branch2c         (Conv2D)
bn4f_branch2c          (BatchNorm)
res5a_branch2a         (Conv2D)
bn5a_branch2a          (BatchNorm)
res5a_branch2b         (Conv2D)
bn5a_branch2b          (BatchNorm)
res5a_branch2c         (Conv2D)
res5a_branch1          (Conv2D)
bn5a_branch2c          (BatchNorm)
bn5a_branch1           (BatchNorm)
res5b_branch2a         (Conv2D)
bn5b_branch2a          (BatchNorm)
res5b_branch2b         (Conv2D)
bn5b_branch2b          (BatchNorm)
res5b_branch2c         (Conv2D)
bn5b_branch2c          (BatchNorm)
res5c_branch2a         (Conv2D)
bn5c_branch2a          (BatchNorm)
res5c_branch2b         (Conv2D)
bn5c_branch2b          (BatchNorm)
res5c_branch2c         (Conv2D)
bn5c_branch2c          (BatchNorm)
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    grasp_rpn_conv_shared   (Conv2D)
    grasp_rpn_class_raw_1   (Conv2D)
    grasp_rpn_class_raw_2   (Conv2D)
    grasp_rpn_bbox_pred_1   (Conv2D)
    grasp_rpn_bbox_pred_2   (Conv2D)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-07-25 00:31:27.146656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-25 00:31:31.404809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
  2/309 [..............................] - ETA: 1:57:28 - loss: 0.0507 - rpn_loss: 0.0506/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.562970). Check your callbacks.
  % (hook_name, delta_t_median), RuntimeWarning)
309/309 [==============================] - 150s 486ms/step - loss: 0.0525 - rpn_loss: 0.0524 - val_loss: 0.0664 - val_rpn_loss: 0.0565
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
309/309 [==============================] - 131s 423ms/step - loss: 0.0521 - rpn_loss: 0.0520 - val_loss: 0.0629 - val_rpn_loss: 0.0561
Epoch 3/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0505 - rpn_loss: 0.0504 - val_loss: 0.0598 - val_rpn_loss: 0.0608
Epoch 4/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0517 - rpn_loss: 0.0516 - val_loss: 0.0538 - val_rpn_loss: 0.0588
Epoch 5/500
309/309 [==============================] - 102s 329ms/step - loss: 0.0490 - rpn_loss: 0.0489 - val_loss: 0.0629 - val_rpn_loss: 0.0564
Epoch 6/500
309/309 [==============================] - 120s 389ms/step - loss: 0.0499 - rpn_loss: 0.0499 - val_loss: 0.0551 - val_rpn_loss: 0.0588
Epoch 7/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0497 - rpn_loss: 0.0496 - val_loss: 0.0543 - val_rpn_loss: 0.0586
Epoch 8/500
309/309 [==============================] - 133s 432ms/step - loss: 0.0495 - rpn_loss: 0.0494 - val_loss: 0.0570 - val_rpn_loss: 0.0559
Epoch 9/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0508 - rpn_loss: 0.0508 - val_loss: 0.0544 - val_rpn_loss: 0.0555
Epoch 10/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0487 - rpn_loss: 0.0486 - val_loss: 0.0535 - val_rpn_loss: 0.0544
Epoch 11/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0485 - rpn_loss: 0.0484 - val_loss: 0.0614 - val_rpn_loss: 0.0573
Epoch 12/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0490 - rpn_loss: 0.0489 - val_loss: 0.0569 - val_rpn_loss: 0.0597
Epoch 13/500
309/309 [==============================] - 133s 431ms/step - loss: 0.0492 - rpn_loss: 0.0491 - val_loss: 0.0578 - val_rpn_loss: 0.0587
Epoch 14/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0485 - rpn_loss: 0.0485 - val_loss: 0.0530 - val_rpn_loss: 0.0549
Epoch 15/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0483 - rpn_loss: 0.0482 - val_loss: 0.0570 - val_rpn_loss: 0.0534
Epoch 16/500
309/309 [==============================] - 137s 443ms/step - loss: 0.0487 - rpn_loss: 0.0486 - val_loss: 0.0532 - val_rpn_loss: 0.0591
Epoch 17/500
309/309 [==============================] - 144s 467ms/step - loss: 0.0485 - rpn_loss: 0.0484 - val_loss: 0.0614 - val_rpn_loss: 0.0575
Epoch 18/500
309/309 [==============================] - 149s 483ms/step - loss: 0.0478 - rpn_loss: 0.0477 - val_loss: 0.0569 - val_rpn_loss: 0.0572
Epoch 19/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0482 - rpn_loss: 0.0482 - val_loss: 0.0673 - val_rpn_loss: 0.0571
Epoch 20/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0475 - rpn_loss: 0.0474 - val_loss: 0.0579 - val_rpn_loss: 0.0573
Epoch 21/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0477 - rpn_loss: 0.0476 - val_loss: 0.0498 - val_rpn_loss: 0.0568
Epoch 22/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0476 - rpn_loss: 0.0475 - val_loss: 0.0504 - val_rpn_loss: 0.0571
Epoch 23/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0480 - rpn_loss: 0.0479 - val_loss: 0.0633 - val_rpn_loss: 0.0564
Epoch 24/500
309/309 [==============================] - 139s 451ms/step - loss: 0.0473 - rpn_loss: 0.0472 - val_loss: 0.0439 - val_rpn_loss: 0.0560
Epoch 25/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0475 - rpn_loss: 0.0474 - val_loss: 0.0502 - val_rpn_loss: 0.0567
Epoch 26/500
309/309 [==============================] - 135s 436ms/step - loss: 0.0474 - rpn_loss: 0.0474 - val_loss: 0.0588 - val_rpn_loss: 0.0573
Epoch 27/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0476 - rpn_loss: 0.0475 - val_loss: 0.0585 - val_rpn_loss: 0.0569
Epoch 28/500
309/309 [==============================] - 137s 444ms/step - loss: 0.0474 - rpn_loss: 0.0473 - val_loss: 0.0589 - val_rpn_loss: 0.0580
Epoch 29/500
309/309 [==============================] - 143s 462ms/step - loss: 0.0468 - rpn_loss: 0.0467 - val_loss: 0.0576 - val_rpn_loss: 0.0559
Epoch 30/500
309/309 [==============================] - 143s 463ms/step - loss: 0.0475 - rpn_loss: 0.0474 - val_loss: 0.0589 - val_rpn_loss: 0.0578
Epoch 31/500
309/309 [==============================] - 143s 462ms/step - loss: 0.0472 - rpn_loss: 0.0471 - val_loss: 0.0494 - val_rpn_loss: 0.0549
Epoch 32/500
309/309 [==============================] - 136s 439ms/step - loss: 0.0469 - rpn_loss: 0.0468 - val_loss: 0.0558 - val_rpn_loss: 0.0573
Epoch 33/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0466 - rpn_loss: 0.0466 - val_loss: 0.0617 - val_rpn_loss: 0.0561
Epoch 34/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0466 - rpn_loss: 0.0465 - val_loss: 0.0508 - val_rpn_loss: 0.0570
Epoch 35/500
309/309 [==============================] - 136s 441ms/step - loss: 0.0457 - rpn_loss: 0.0456 - val_loss: 0.0572 - val_rpn_loss: 0.0580
Epoch 36/500
309/309 [==============================] - 135s 438ms/step - loss: 0.0462 - rpn_loss: 0.0461 - val_loss: 0.0530 - val_rpn_loss: 0.0570
Epoch 37/500
309/309 [==============================] - 132s 429ms/step - loss: 0.0462 - rpn_loss: 0.0461 - val_loss: 0.0496 - val_rpn_loss: 0.0571
Epoch 38/500
309/309 [==============================] - 134s 435ms/step - loss: 0.0464 - rpn_loss: 0.0463 - val_loss: 0.0536 - val_rpn_loss: 0.0553
Epoch 39/500
309/309 [==============================] - 134s 433ms/step - loss: 0.0462 - rpn_loss: 0.0461 - val_loss: 0.0672 - val_rpn_loss: 0.0571
Epoch 40/500
309/309 [==============================] - 134s 434ms/step - loss: 0.0463 - rpn_loss: 0.0462 - val_loss: 0.0593 - val_rpn_loss: 0.0584
Epoch 41/500
309/309 [==============================] - 137s 445ms/step - loss: 0.0462 - rpn_loss: 0.0461 - val_loss: 0.0523 - val_rpn_loss: 0.0571
Epoch 42/500
309/309 [==============================] - 147s 474ms/step - loss: 0.0452 - rpn_loss: 0.0451 - val_loss: 0.0582 - val_rpn_loss: 0.0562
Epoch 43/500
123/309 [==========>...................] - ETA: 1:37 - loss: 0.0455 - rpn_loss: 0.0454
